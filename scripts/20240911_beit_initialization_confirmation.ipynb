{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/austinwang/unilm/beit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/austinwang/unilm/beit/modeling_finetune.py:379: UserWarning: Overwriting beit_base_patch16_224 in registry with modeling_finetune.beit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/austinwang/unilm/beit/modeling_finetune.py:388: UserWarning: Overwriting beit_base_patch16_384 in registry with modeling_finetune.beit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_base_patch16_384(pretrained=False, **kwargs):\n",
      "/home/austinwang/unilm/beit/modeling_finetune.py:397: UserWarning: Overwriting beit_large_patch16_224 in registry with modeling_finetune.beit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_large_patch16_224(pretrained=False, **kwargs):\n",
      "/home/austinwang/unilm/beit/modeling_finetune.py:406: UserWarning: Overwriting beit_large_patch16_384 in registry with modeling_finetune.beit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_large_patch16_384(pretrained=False, **kwargs):\n",
      "/home/austinwang/unilm/beit/modeling_finetune.py:415: UserWarning: Overwriting beit_large_patch16_512 in registry with modeling_finetune.beit_large_patch16_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def beit_large_patch16_512(pretrained=False, **kwargs):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 197, 768]),\n",
       " VisionTransformerForMaskedImageModeling(\n",
       "   (patch_embed): PatchEmbed(\n",
       "     (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "   )\n",
       "   (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "   (blocks): ModuleList(\n",
       "     (0-11): 12 x Block(\n",
       "       (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "       (attn): Attention(\n",
       "         (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "         (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "         (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "       (drop_path): Identity()\n",
       "       (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "       (mlp): Mlp(\n",
       "         (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "         (act): GELU(approximate='none')\n",
       "         (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "         (drop): Dropout(p=0.0, inplace=False)\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "   (lm_head): Linear(in_features=768, out_features=8192, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %cd ~/InternVL/classification/\n",
    "# from models.build import build_model\n",
    "# # initialize config\n",
    "# import ml_collections\n",
    "# config_dict = {\n",
    "#     \"MODEL\": {\n",
    "#         \"TYPE\": \"intern_vit_6b\",\n",
    "#         \"NUM_CLASSES\": 0,\n",
    "#         \"DROP_PATH_RATE\": 0.0,\n",
    "#         \"INTERN_VIT_6B\": {\n",
    "#             \"PATCH_SIZE\": 16,\n",
    "#             \"EMBED_DIM\": 768,\n",
    "#             \"NUM_HEADS\": 12,\n",
    "#             \"MLP_RATIO\": 4,\n",
    "#             \"QKV_BIAS\": True,\n",
    "#             \"INIT_VALUES\": 1e-6,\n",
    "#             \"QK_NORMALIZATION\": False,\n",
    "#             \"DEPTH\": 12,\n",
    "#             \"USE_FLASH_ATTN\": False,\n",
    "#             \"PRETRAIN_SIZE\": 224,\n",
    "#             \"PRETRAINED\": None,\n",
    "#             \"CLS_TARGET\": \"clip_projector\",\n",
    "#             \"HEAD_NORM_TYPE\": \"none\",\n",
    "#             \"FREEZE_VIT\": False,\n",
    "#         },\n",
    "#     },\n",
    "#     \"DATA\": {\n",
    "#         \"IMG_SIZE\": 224\n",
    "#     },\n",
    "#     \"TRAIN\": {\n",
    "#         \"USE_CHECKPOINT\": False\n",
    "#     }\n",
    "# }\n",
    "# config = ml_collections.ConfigDict(config_dict)\n",
    "# model = build_model(config=config)\n",
    "%cd ~/unilm/beit/\n",
    "from modeling_pretrain import beit_base_patch16_224_8k_vocab\n",
    "model = beit_base_patch16_224_8k_vocab(pretrained=False)\n",
    "# Input Initialization\n",
    "dummy_img = torch.randn(1, 3, 224, 224)\n",
    "bool_masked_pos = torch.randn(1,196).bool()\n",
    "out = model.forward_features(dummy_img,bool_masked_pos)\n",
    "out.shape, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trucated normalization comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "class flax_foo(flax.linen.Module):\n",
    "    feature_dim: int\n",
    "    kernel_init: flax.linen.initializers.Initializer = flax.linen.initializers.truncated_normal(0.02, dtype=jnp.float32, lower=-0.02, upper=0.02)\n",
    "    bias_init: flax.linen.initializers.Initializer = flax.linen.initializers.zeros\n",
    "    def setup(self):\n",
    "        self.dense = flax.linen.Dense(features=self.feature_dim, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
    "    def __call__(self, x):\n",
    "        return self.dense(x)\n",
    "\n",
    "# torch \n",
    "from timm.models.layers import trunc_normal_\n",
    "def trunc_normal(param): return trunc_normal_(param, mean=0.0, std=0.02, a=-0.02, b=0.02)\n",
    "class torch_foo(torch.nn.Module):\n",
    "    def __init__(self, feature_dim: int):\n",
    "        super().__init__()\n",
    "        self.dense = torch.nn.Linear(feature_dim, feature_dim)\n",
    "        trunc_normal(self.dense.weight)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Initialization\n",
    "feature_dim = 3\n",
    "x = torch.randn(1, feature_dim, requires_grad=False)\n",
    "x_flax = jnp.array(x)\n",
    "\n",
    "# model initialization\n",
    "flax_model = flax_foo(feature_dim)\n",
    "params = flax_model.init(jax.random.PRNGKey(0), x_flax)\n",
    "torch_model = torch_foo(feature_dim)\n",
    "torch_params = torch_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[-4.0258612e-05,  2.2024987e-04, -1.4944360e-04],\n",
       "        [-3.4164346e-04,  3.4291650e-05, -3.5881568e-04],\n",
       "        [-8.0378231e-05, -2.9115484e-04, -1.3574921e-04]], dtype=float32),\n",
       " tensor([[-0.0051,  0.0075,  0.0189],\n",
       "         [-0.0121,  0.0095,  0.0100],\n",
       "         [-0.0084, -0.0161,  0.0034]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['params']['dense']['kernel'], torch_params['dense.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001775238779373467, 0.011768573895096779)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['params']['dense']['kernel'].std().item(), torch_params['dense.weight'].std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# forward pass\n",
    "flax_out = flax_model.apply(params, x_flax)\n",
    "torch_out = torch_model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
