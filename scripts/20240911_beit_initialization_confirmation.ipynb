{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/austinwang/unilm/beit/'\n",
      "/home/austinwang/austin_big_vision/scripts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modeling_pretrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %cd ~/InternVL/classification/\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# from models.build import build_model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# # initialize config\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# config = ml_collections.ConfigDict(config_dict)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# model = build_model(config=config)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/unilm/beit/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodeling_pretrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m beit_base_patch16_224_8k_vocab\n\u001b[1;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m beit_base_patch16_224_8k_vocab(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Input Initialization\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'modeling_pretrain'"
     ]
    }
   ],
   "source": [
    "# %cd ~/InternVL/classification/\n",
    "# from models.build import build_model\n",
    "# # initialize config\n",
    "# import ml_collections\n",
    "# config_dict = {\n",
    "#     \"MODEL\": {\n",
    "#         \"TYPE\": \"intern_vit_6b\",\n",
    "#         \"NUM_CLASSES\": 0,\n",
    "#         \"DROP_PATH_RATE\": 0.0,\n",
    "#         \"INTERN_VIT_6B\": {\n",
    "#             \"PATCH_SIZE\": 16,\n",
    "#             \"EMBED_DIM\": 768,\n",
    "#             \"NUM_HEADS\": 12,\n",
    "#             \"MLP_RATIO\": 4,\n",
    "#             \"QKV_BIAS\": True,\n",
    "#             \"INIT_VALUES\": 1e-6,\n",
    "#             \"QK_NORMALIZATION\": False,\n",
    "#             \"DEPTH\": 12,\n",
    "#             \"USE_FLASH_ATTN\": False,\n",
    "#             \"PRETRAIN_SIZE\": 224,\n",
    "#             \"PRETRAINED\": None,\n",
    "#             \"CLS_TARGET\": \"clip_projector\",\n",
    "#             \"HEAD_NORM_TYPE\": \"none\",\n",
    "#             \"FREEZE_VIT\": False,\n",
    "#         },\n",
    "#     },\n",
    "#     \"DATA\": {\n",
    "#         \"IMG_SIZE\": 224\n",
    "#     },\n",
    "#     \"TRAIN\": {\n",
    "#         \"USE_CHECKPOINT\": False\n",
    "#     }\n",
    "# }\n",
    "# config = ml_collections.ConfigDict(config_dict)\n",
    "# model = build_model(config=config)\n",
    "%cd ~/unilm/beit/\n",
    "from modeling_pretrain import beit_base_patch16_224_8k_vocab\n",
    "model = beit_base_patch16_224_8k_vocab(pretrained=False)\n",
    "# Input Initialization\n",
    "dummy_img = torch.randn(1, 3, 224, 224)\n",
    "bool_masked_pos = torch.randn(1,196).bool()\n",
    "out = model.forward_features(dummy_img,bool_masked_pos)\n",
    "out.shape, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trucated normalization comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Flax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "class flax_foo(flax.linen.Module):\n",
    "    feature_dim: int\n",
    "    kernel_init: flax.linen.initializers.Initializer = flax.linen.initializers.truncated_normal(1.0, dtype=jnp.float32, lower=-0.02, upper=0.02)\n",
    "    bias_init: flax.linen.initializers.Initializer = flax.linen.initializers.zeros\n",
    "    def setup(self):\n",
    "        self.dense = flax.linen.Dense(features=self.feature_dim, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
    "    def __call__(self, x):\n",
    "        return self.dense(x)\n",
    "\n",
    "# torch \n",
    "from timm.models.layers import trunc_normal_\n",
    "def trunc_normal(param): return trunc_normal_(param, mean=0.0, std=0.02, a=-0.02, b=0.02)\n",
    "class torch_foo(torch.nn.Module):\n",
    "    def __init__(self, feature_dim: int):\n",
    "        super().__init__()\n",
    "        self.dense = torch.nn.Linear(feature_dim, feature_dim)\n",
    "        trunc_normal(self.dense.weight)\n",
    "        torch.nn.init.zeros_(self.dense.bias)\n",
    "    def forward(self, x):\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Initialization\n",
    "feature_dim = 2048\n",
    "torch.manual_seed(0)\n",
    "x = torch.ones(1, feature_dim, requires_grad=False)\n",
    "x_flax = jnp.array(x)\n",
    "\n",
    "# model initialization\n",
    "flax_model = flax_foo(feature_dim)\n",
    "params = flax_model.init(jax.random.PRNGKey(0), x_flax)\n",
    "torch_model = torch_foo(feature_dim)\n",
    "torch_params = torch_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params['params']['dense']['kernel'], torch_params['dense.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01154741458594799, 0.010793177410960197)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['params']['dense']['kernel'].std().item(), torch_params['dense.weight'].std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "flax_out = flax_model.apply(params, x_flax)\n",
    "torch_out = torch_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
