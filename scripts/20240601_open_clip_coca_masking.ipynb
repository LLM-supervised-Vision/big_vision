{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text.shape: torch.Size([10, 6])\n",
      "text: tensor([[ 44,  47,   1,   0,   0,   0],\n",
      "        [ 67, 103,   9,   1,   0,   0],\n",
      "        [ 36,  87,  70,  88,  88,   1],\n",
      "        [  1,   0,   0,   0,   0,   0],\n",
      "        [ 88,   1,   0,   0,   0,   0],\n",
      "        [  9,  20, 115,   1,   0,   0],\n",
      "        [126,   1,   0,   0,   0,   0],\n",
      "        [ 88,   1,   0,   0,   0,   0],\n",
      "        [ 14,   1,   0,   0,   0,   0],\n",
      "        [127,  32,  31,   1,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "random_seed = 0\n",
    "num_pos = 6 # 64\n",
    "pad_id = 0\n",
    "eos_id = 1\n",
    "heads = 8\n",
    "vocab_size = 128 # 64000\n",
    "width = 768\n",
    "batch_size = 10\n",
    "\n",
    "# initialization\n",
    "token_embedding = nn.Embedding(vocab_size, width)\n",
    "cls_emb = nn.Parameter(torch.empty(width))\n",
    "nn.init.normal_(cls_emb, std=0.01)\n",
    "\n",
    "# create random text input\n",
    "text = torch.randint(0, vocab_size, (batch_size, num_pos),generator=torch.Generator().manual_seed(random_seed))\n",
    "eos_indices = torch.randint(0, num_pos, (batch_size, 1), generator=torch.Generator().manual_seed(random_seed))\n",
    "text = text.scatter_(1, eos_indices, eos_id)\n",
    "for i in range(batch_size): text[i, eos_indices[i, 0]+1:] = pad_id\n",
    "print(f\"text.shape: {text.shape}\")\n",
    "print(f\"text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_causal_mask():\n",
    "\t# lazily create causal attention mask, with full attention between the tokens\n",
    "\t# pytorch uses additive attention mask; fill with -inf\n",
    "\tmask = torch.empty(num_pos+1, num_pos+1)\n",
    "\tmask.fill_(float(\"-inf\"))\n",
    "\tmask.triu_(1)  # zero out the lower diagonal\n",
    "\treturn mask\n",
    "\n",
    "def _expand_token(token, batch_size: int):\n",
    "    return token.view(1, 1, -1).expand(batch_size, -1, -1)\n",
    "\n",
    "def build_cls_mask(text, cast_dtype: torch.dtype):\n",
    "\tcls_mask = (text != pad_id).unsqueeze(1) # (batch_size, 1, num_pos)\n",
    "\tcls_mask = F.pad(cls_mask, (0, 1, cls_mask.shape[2], 0), value=True) # (batch_size, num_pos+1, num_pos+1)\n",
    "\tadditive_mask = torch.empty(cls_mask.shape, dtype=cast_dtype, device=cls_mask.device)\n",
    "\tadditive_mask.fill_(0)\n",
    "\tadditive_mask.masked_fill_(~cls_mask, float(\"-inf\"))\n",
    "\tadditive_mask = torch.repeat_interleave(additive_mask, heads, 0)\n",
    "\treturn additive_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = text.shape[1] # num_pos\n",
    "x = token_embedding(text) # (batch_size, num_pos, width)\n",
    "attn_mask = build_causal_mask() # (num_pos, num_pos)\n",
    "\n",
    "# appending cls_emb to the txt_emb\n",
    "seq_len += 1 # num_pos+1\n",
    "x = torch.cat([x, _expand_token(cls_emb, x.shape[0])], dim=1) # (batch_size, num_pos+1, width)\n",
    "cls_mask = build_cls_mask(text, x.dtype) # (batch_size, num_pos+1, num_pos+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 67, 103,   9,   1,   0,   0]),\n",
       " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., -inf, -inf, -inf, 0.]]),\n",
       " tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, 0.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[1],attn_mask,cls_mask[1],attn_mask+cls_mask[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7, 7]), torch.Size([80, 7, 7]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask[None, :seq_len, :seq_len].shape, cls_mask[:, :seq_len, :seq_len].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_attn_mask = attn_mask[None, :seq_len, :seq_len] + cls_mask[:, :seq_len, :seq_len] # (batch_size, num_pos+1, num_pos+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 7, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# build_cls_mask\\nprint(f\"text.shape = {text.shape}\")\\nprint()\\ncls_mask = (text != pad_id).unsqueeze(1)\\nprint(f\"cls_mask.shape = {cls_mask.shape}\")\\nprint(f\"cls_mask[0] = {cls_mask[0]}\")\\nprint()\\ncls_mask = F.pad(cls_mask, (1, 0, cls_mask.shape[2], 0), value=True)\\nprint(f\"cls_mask.shape = {cls_mask.shape}\")\\nprint(f\"cls_mask[0] = {cls_mask[0]}\")\\nprint()\\nadditive_mask = torch.empty(cls_mask.shape, device=cls_mask.device)\\nadditive_mask.fill_(0)\\nadditive_mask.masked_fill_(~cls_mask, float(\"-inf\"))\\nprint(f\"additive_mask.shape = {additive_mask.shape}\")\\nprint(f\"additive_mask[0] = {additive_mask[0]}\")\\nprint()\\nadditive_mask = torch.repeat_interleave(additive_mask, heads, 0)\\nprint(f\"additive_mask.shape = {additive_mask.shape}\")\\nprint(f\"additive_mask[0] = {additive_mask[0]}\")\\nprint()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# build_cls_mask\n",
    "print(f\"text.shape = {text.shape}\")\n",
    "print()\n",
    "cls_mask = (text != pad_id).unsqueeze(1)\n",
    "print(f\"cls_mask.shape = {cls_mask.shape}\")\n",
    "print(f\"cls_mask[0] = {cls_mask[0]}\")\n",
    "print()\n",
    "cls_mask = F.pad(cls_mask, (1, 0, cls_mask.shape[2], 0), value=True)\n",
    "print(f\"cls_mask.shape = {cls_mask.shape}\")\n",
    "print(f\"cls_mask[0] = {cls_mask[0]}\")\n",
    "print()\n",
    "additive_mask = torch.empty(cls_mask.shape, device=cls_mask.device)\n",
    "additive_mask.fill_(0)\n",
    "additive_mask.masked_fill_(~cls_mask, float(\"-inf\"))\n",
    "print(f\"additive_mask.shape = {additive_mask.shape}\")\n",
    "print(f\"additive_mask[0] = {additive_mask[0]}\")\n",
    "print()\n",
    "additive_mask = torch.repeat_interleave(additive_mask, heads, 0)\n",
    "print(f\"additive_mask.shape = {additive_mask.shape}\")\n",
    "print(f\"additive_mask[0] = {additive_mask[0]}\")\n",
    "print()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
