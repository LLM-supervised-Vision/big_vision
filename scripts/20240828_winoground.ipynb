{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/austinwang/austin_big_vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 21:55:05.418873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-28 21:55:05.441236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-28 21:55:05.448093: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-28 21:55:06.494231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ~/austin_big_vision\n",
    "import io\n",
    "import os\n",
    "import jax\n",
    "import json\n",
    "import torch\n",
    "import importlib\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "import jax.numpy as jnp\n",
    "import tensorflow as tf\n",
    "\n",
    "import open_clip\n",
    "import big_vision.utils as u\n",
    "import big_vision.models.vit as model_mod\n",
    "from big_vision.models.vit import scan_to_pyloop\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "backbone = 'gemma2b-half-0.1_b16-F_contrastive'\n",
    "\n",
    "# setup\n",
    "backbone_dict = {\n",
    "    'clip': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_bs16384_warm10k_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_07-23_1510',\n",
    "    'clip_map': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_autoregressive_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019',\n",
    "    'clip_s9b': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_autoregressive_s9b_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_08-04_0839',\n",
    "    'clip_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-09_0655',\n",
    "    'siglip': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_parallel_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019',\n",
    "    'siglip_v4-32': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_replication_pod_04-11_2247',\n",
    "    'siglip_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-04_0839',\n",
    "    'cappa': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/cappa_bs16384_s3B_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-27_2108',\n",
    "    # 'cappa_decoder-qknorm-T_warm0.02': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_warm0.02_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-15_2102',\n",
    "    'cappa_s9b': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_s9B_warm0.02_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-27_2108',\n",
    "    'cappa_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs32768_s9B_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_08-07_2217',\n",
    "    'coca_6lyr': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_bs16384_warm0.03_1.0co-2.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-30_1841',\n",
    "    'coca_unified': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_s3b_bs16384_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-19_0355',\n",
    "    'coca_1.0co_1.0ca_6lyr_qknorm-T_warm0.02': 'gs://us-central2-storage/tensorflow_datasets/ckpts/coca_replication_bs16384_warm0.02_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-10_2225',\n",
    "    'coca': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-12_2313',\n",
    "    'coca_6lyr_1.0co_1.0ca_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_s9b_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-14_1614',\n",
    "    'gemma2b-half-0.1_b16-F_contrastive': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-half-0.1_so400m-F_contrastive_bs16384_s3b_wd1e-4_08-21_1935',\n",
    "}\n",
    "backbone_path = backbone_dict[backbone]\n",
    "config_path = f'{backbone_path}/config.json'\n",
    "config = ml_collections.ConfigDict(json.load(tf.io.gfile.GFile(config_path, \"r\")))\n",
    "ckpt_path = f'{backbone_path}/checkpoint.bv-{config.total_steps:09d}'\n",
    "\n",
    "# load model\n",
    "model_cfg = config.model\n",
    "img_key = 'img' if 'image' in model_cfg or 'img' in model_cfg else 'encoder'\n",
    "model_mod = importlib.import_module(f\"big_vision.models.{config.model_name}\")\n",
    "bv_model = model_mod.Model(**model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724882163.012407  194483 gcs_resource.cc:109] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1724882163.019526  196423 google_auth_provider.cc:180] Running on GCE, using service account 373177222751-compute@developer.gserviceaccount.com\n",
      "/tmp/ipykernel_194483/4280382188.py:13: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  jax.tree_map(jnp.shape, params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'img': {'Transformer': {'encoder_norm': {'bias': (768,), 'scale': (768,)},\n",
       "   'encoderblock': {'LayerNorm_0': {'bias': (12, 768), 'scale': (12, 768)},\n",
       "    'LayerNorm_1': {'bias': (12, 768), 'scale': (12, 768)},\n",
       "    'MlpBlock_0': {'Dense_0': {'bias': (12, 3072), 'kernel': (12, 768, 3072)},\n",
       "     'Dense_1': {'bias': (12, 768), 'kernel': (12, 3072, 768)}},\n",
       "    'MultiHeadDotProductAttention_0': {'key': {'bias': (12, 12, 64),\n",
       "      'kernel': (12, 768, 12, 64)},\n",
       "     'out': {'bias': (12, 768), 'kernel': (12, 12, 64, 768)},\n",
       "     'query': {'bias': (12, 12, 64), 'kernel': (12, 768, 12, 64)},\n",
       "     'value': {'bias': (12, 12, 64), 'kernel': (12, 768, 12, 64)}}}},\n",
       "  'embedding': {'bias': (768,), 'kernel': (16, 16, 3, 768)},\n",
       "  'head': {'bias': (2048,), 'kernel': (768, 2048)},\n",
       "  'pos_embedding': (1, 196, 768)},\n",
       " 'llm': {'embedder': {'input_embedding': (257152, 2048)},\n",
       "  'final_norm': {'scale': (2048,)},\n",
       "  'layers': {'attn': {'attn_vec_einsum': {'w': (9, 8, 256, 2048)},\n",
       "    'kv_einsum': {'w': (9, 2, 1, 2048, 256)},\n",
       "    'q_einsum': {'w': (9, 8, 2048, 256)}},\n",
       "   'mlp': {'gating_einsum': (9, 2, 2048, 16384), 'linear': (9, 16384, 2048)},\n",
       "   'pre_attention_norm': {'scale': (9, 2048)},\n",
       "   'pre_ffw_norm': {'scale': (9, 2048)}}},\n",
       " 't': (1,)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ckpt weights\n",
    "rng = jax.random.PRNGKey(42)\n",
    "dummy_img = jnp.zeros([2, 224, 224, 3], jnp.float32)\n",
    "dummy_txt = jnp.zeros([2, 64], jnp.int32)\n",
    "dummy_mask_ar = jnp.zeros([2, 64], jnp.bool_) if 'llm' in model_cfg else None\n",
    "if dummy_mask_ar is not None:\n",
    "    init_params = jax.jit(bv_model.init, backend=\"cpu\")(rng, dummy_img, dummy_txt,dummy_mask_ar)['params']\n",
    "else:\n",
    "    init_params = jax.jit(bv_model.init, backend=\"cpu\")(rng, dummy_img, dummy_txt)['params']\n",
    "\n",
    "img_load_kw = {'dont_load': ('.*_ln/scale','head/kernel', 'head/bias')}\n",
    "params = model_mod.load(init_params, ckpt_path, model_cfg, img_load_kw)\n",
    "jax.tree_map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winoground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/datasets/load.py:2566: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "examples = load_dataset('facebook/winoground', use_auth_token=\"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\",trust_remote_code=True)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import big_vision.pp.builder as pp_builder\n",
    "for m in config.get(\"pp_modules\", [\"ops_general\", \"ops_image\", \"ops_text\"]): importlib.import_module(f\"big_vision.pp.{m}\")\n",
    "\n",
    "pp_img_idx = config.input.pp.split('|').index('value_range(-1,1)')\n",
    "pp_img = pp_builder.get_preprocess_fn('|'.join(config.input.pp.split('|')[1:pp_img_idx+1]))\n",
    "pp_txt = pp_builder.get_preprocess_fn('|'.join(config.input.pp.split('|')[pp_img_idx+1:]))\n",
    "pp = pp_builder.get_preprocess_fn(config.input.pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i0_c0: -0.002773693995550275\n",
      "i0_c1: -0.00036525039467960596\n",
      "i1_c0: 0.009745581075549126\n",
      "i1_c1: 0.015098599717020988\n"
     ]
    }
   ],
   "source": [
    "example = examples[0]\n",
    "image_0 = pp_img({\"image\": jnp.asarray(example['image_0'])})['image'][None,:]\n",
    "image_1 = pp_img({\"image\": jnp.asarray(example['image_1'])})['image'][None,:]\n",
    "caption_0 = pp_txt({\"caption\": example['caption_0']})\n",
    "caption_1 = pp_txt({\"caption\": example['caption_1']})\n",
    "\n",
    "zimg_0, out_img_0 = bv_model.apply({\"params\":params}, image_0, method='embed_image')\n",
    "zimg_1, out_img_1 = bv_model.apply({\"params\":params}, image_1, method='embed_image')\n",
    "ztxt_0 = bv_model.apply(\n",
    "    {\"params\":params}, \n",
    "    None, jnp.array(caption_0['text'][None,:-1]), \n",
    "    jnp.array(caption_0['mask_ar'][None,:-1]), \n",
    "    is_blind=True\n",
    ")[1]['llm/pre_logits']\n",
    "ztxt_1 = bv_model.apply(\n",
    "    {\"params\":params}, \n",
    "    None, jnp.array(caption_1['text'][None,:-1]), \n",
    "    jnp.array(caption_1['mask_ar'][None,:-1]), \n",
    "    is_blind=True\n",
    ")[1]['llm/pre_logits']\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    if a.ndim == 2: \n",
    "        a = a[None]\n",
    "        b = b[None]\n",
    "    def sim(a_i, b_i):\n",
    "        return np.dot(a_i, b_i) / (np.linalg.norm(a_i) * np.linalg.norm(b_i))\n",
    "    sim_list = [sim(a_i, b_i) for a_i, b_i in zip(a[0], b[0])]\n",
    "    return np.mean(sim_list)\n",
    "\n",
    "print(f\"i0_c0: {cosine_similarity(zimg_0, ztxt_0)}\")\n",
    "print(f\"i0_c1: {cosine_similarity(zimg_0, ztxt_1)}\")\n",
    "print(f\"i1_c0: {cosine_similarity(zimg_1, ztxt_0)}\")\n",
    "print(f\"i1_c1: {cosine_similarity(zimg_1, ztxt_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
