{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/austinwang/austin_big_vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ~/austin_big_vision\n",
    "import jax\n",
    "import json\n",
    "import torch\n",
    "import importlib\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "import jax.numpy as jnp\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "\n",
    "import big_vision.utils as u\n",
    "import big_vision.pp.builder as pp_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = load_dataset('facebook/winoground', token=\"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\",trust_remote_code=True)['test']\n",
    "\n",
    "def compute_winoground_scores(i0_c0,i0_c1,i1_c0,i1_c1):\n",
    "    print(\"Computing Winoground scores\")\n",
    "    # text scores: 1 if i0_c0 > i0_c1 and i1_c1 > i1_c0, 0 otherwise\n",
    "    # image scores: 1 if i0_c0 > i1_c0 and i1_c1 > i0_c1, 0 otherwise\n",
    "    text_scores = (i0_c0 > i0_c1) & (i1_c1 > i1_c0)\n",
    "    image_scores = (i0_c0 > i1_c0) & (i1_c1 > i0_c1)\n",
    "    both_scores = text_scores & image_scores\n",
    "    def get_acc(scores): \n",
    "        # return eg.72.50%\n",
    "        s = scores.mean().item()*100\n",
    "        return f'{s:.2f}'\n",
    "    print(\"Text Score:\", get_acc(text_scores))\n",
    "    print(\"Image Score:\", get_acc(image_scores))\n",
    "    print(\"Both Score:\", get_acc(both_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big_Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading weights from gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-09_0655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412090/1932418312.py:54: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  jax.tree_map(jnp.shape, params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'img': {'Transformer': {'encoder_norm': {'bias': (768,), 'scale': (768,)},\n",
       "   'encoderblock': {'LayerNorm_0': {'bias': (12, 768), 'scale': (12, 768)},\n",
       "    'LayerNorm_1': {'bias': (12, 768), 'scale': (12, 768)},\n",
       "    'MlpBlock_0': {'Dense_0': {'bias': (12, 3072), 'kernel': (12, 768, 3072)},\n",
       "     'Dense_1': {'bias': (12, 768), 'kernel': (12, 3072, 768)}},\n",
       "    'MultiHeadDotProductAttention_0': {'key': {'bias': (12, 12, 64),\n",
       "      'kernel': (12, 768, 12, 64)},\n",
       "     'out': {'bias': (12, 768), 'kernel': (12, 12, 64, 768)},\n",
       "     'query': {'bias': (12, 12, 64), 'kernel': (12, 768, 12, 64)},\n",
       "     'value': {'bias': (12, 12, 64), 'kernel': (12, 768, 12, 64)}}}},\n",
       "  'embedding': {'bias': (768,), 'kernel': (16, 16, 3, 768)},\n",
       "  'head': {'kernel': (768, 768)},\n",
       "  'pos_embedding': (1, 196, 768)},\n",
       " 't': (1,),\n",
       " 'txt': {'Embed_0': {'embedding': (32000, 768)},\n",
       "  'Encoder_0': {'encoder_norm': {'bias': (768,), 'scale': (768,)},\n",
       "   'encoderblock': {'LayerNorm_0': {'bias': (12, 768), 'scale': (12, 768)},\n",
       "    'LayerNorm_1': {'bias': (12, 768), 'scale': (12, 768)},\n",
       "    'MlpBlock_0': {'Dense_0': {'bias': (12, 3072), 'kernel': (12, 768, 3072)},\n",
       "     'Dense_1': {'bias': (12, 768), 'kernel': (12, 3072, 768)}},\n",
       "    'MultiHeadDotProductAttention_0': {'key': {'bias': (12, 12, 64),\n",
       "      'kernel': (12, 768, 12, 64)},\n",
       "     'out': {'bias': (12, 768), 'kernel': (12, 12, 64, 768)},\n",
       "     'query': {'bias': (12, 12, 64), 'kernel': (12, 768, 12, 64)},\n",
       "     'value': {'bias': (12, 12, 64), 'kernel': (12, 768, 12, 64)}}}},\n",
       "  'head': {'kernel': (768, 768)},\n",
       "  'pos_embedding': (1, 64, 768)}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "backbone = 'gemma2b-half-0.1_b16-F_contrastive'\n",
    "backbone = 'clip_s9b_bs32k'\n",
    "\n",
    "# setup\n",
    "backbone_dict = {\n",
    "    'clip': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_bs16384_warm10k_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_07-23_1510',\n",
    "    'clip_replication': 'gs://us-central2-storage/tensorflow_datasets/clip-replication_autoregressive_bs32768_warm0.03_lr5e-4_wd1e-4_bf16_qknorm-F_b1-0.9_b2-0.98_12lyr_06-24_2019',\n",
    "    'clip_map': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_autoregressive_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019',\n",
    "    'clip_s9b': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_autoregressive_s9b_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_08-04_0839',\n",
    "    'clip_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-09_0655',\n",
    "    'siglip': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_parallel_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019',\n",
    "    'siglip_v4-32': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_replication_pod_04-11_2247',\n",
    "    'siglip_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-04_0839',\n",
    "    'cappa': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/cappa_bs16384_s3B_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-27_2108',\n",
    "    # 'cappa_decoder-qknorm-T_warm0.02': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_warm0.02_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-15_2102',\n",
    "    'cappa_s9b': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_s9B_warm0.02_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-27_2108',\n",
    "    'cappa_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs32768_s9B_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_08-07_2217',\n",
    "    'coca_6lyr': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_bs16384_warm0.03_1.0co-2.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-30_1841',\n",
    "    'coca_unified': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_s3b_bs16384_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-19_0355',\n",
    "    'coca_1.0co_1.0ca_6lyr_qknorm-T_warm0.02': 'gs://us-central2-storage/tensorflow_datasets/ckpts/coca_replication_bs16384_warm0.02_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-10_2225',\n",
    "    'coca': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-12_2313',\n",
    "    'coca_6lyr_1.0co_1.0ca_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_s9b_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-14_1614',\n",
    "    'coca_unified_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_unified_s9b_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-20_0246',\n",
    "    'gemma2b-half-0.1_b16-F_contrastive': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-half-0.1_so400m-F_contrastive_bs16384_s3b_wd1e-4_08-21_1935',\n",
    "}\n",
    "backbone_path = backbone_dict[backbone]\n",
    "config_path = f'{backbone_path}/config.json'\n",
    "config = ml_collections.ConfigDict(json.load(tf.io.gfile.GFile(config_path, \"r\")))\n",
    "for m in config.get(\"pp_modules\", [\"ops_general\", \"ops_image\", \"ops_text\"]): importlib.import_module(f\"big_vision.pp.{m}\")\n",
    "\n",
    "# load model\n",
    "print(f\"Loading model\")\n",
    "model_cfg = config.model\n",
    "if 'proj_bias' not in model_cfg.text and backbone.split('_')[0]==\"siglip\": model_cfg.text.proj_bias = True\n",
    "img_key = 'img' if 'image' in model_cfg or 'img' in model_cfg else 'encoder'\n",
    "model_mod = importlib.import_module(f\"big_vision.models.{config.model_name}\")\n",
    "bv_model = model_mod.Model(**model_cfg)\n",
    "\n",
    "# load ckpt weights\n",
    "print(f'Loading weights from {backbone_path}')\n",
    "rng = jax.random.PRNGKey(42)\n",
    "dummy_img = jnp.zeros([2, 224, 224, 3], jnp.float32)\n",
    "dummy_txt = jnp.zeros([2, 64], jnp.int32)\n",
    "dummy_mask_ar = jnp.zeros([2, 64], jnp.bool_) if 'llm' in model_cfg else None\n",
    "if dummy_mask_ar is not None:\n",
    "    init_params = jax.jit(bv_model.init, backend=\"cpu\")(rng, dummy_img, dummy_txt,dummy_mask_ar)['params']\n",
    "else:\n",
    "    init_params = jax.jit(bv_model.init, backend=\"cpu\")(rng, dummy_img, dummy_txt)['params']\n",
    "\n",
    "# img_load_kw = {'dont_load': ('.*_ln/scale','head/kernel', 'head/bias')}\n",
    "ckpt_path = f'{backbone_path}/checkpoint.bv-{config.total_steps:09d}'\n",
    "params = model_mod.load(init_params, ckpt_path, model_cfg) # , img_load_kw)\n",
    "jax.tree_map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing: decode|resize(224)|flip_lr|value_range(-1,1)|tokenize(max_len=64, model=\"c4_en\", clip_bpe=False, eos=\"sticky\", pad_value=1, inkey=\"caption\", outkey=\"labels\")|keep(\"image\", \"labels\")\n",
      "Preprocessing images\n"
     ]
    }
   ],
   "source": [
    "print(f\"Applying preprocessing: {config.input.pp}\")\n",
    "pp_img_idx = config.input.pp.split('|').index('value_range(-1,1)')\n",
    "pp_list = config.input.pp.split('|')\n",
    "assert pp_list[0] == 'decode'\n",
    "pp_img = pp_builder.get_preprocess_fn('|'.join(pp_list[1:pp_img_idx+1]))\n",
    "pp_txt = pp_builder.get_preprocess_fn('|'.join(pp_list[pp_img_idx+1:]))\n",
    "print(\"Preprocessing images\")\n",
    "i0 = jnp.array([pp_img({\"image\": jnp.asarray(img.convert('RGB'))})['image'] for img in examples['image_0']])\n",
    "i1 = jnp.array([pp_img({\"image\": jnp.asarray(img.convert('RGB'))})['image'] for img in examples['image_1']])\n",
    "print(\"Preprocessing text\")\n",
    "c0_list, c0 = [pp_txt({\"caption\": txt}) for txt in examples['caption_0']], {}\n",
    "for k in c0_list[0].keys(): c0[k] = jnp.array([d[k] for d in c0_list])\n",
    "c1_list, c1 = [pp_txt({\"caption\": txt}) for txt in examples['caption_1']], {}\n",
    "for k in c1_list[0].keys(): c1[k] = jnp.array([d[k] for d in c1_list])\n",
    "print(f\"i0.shape: {i0.shape}, c0 shapes: {jax.tree.map(lambda x: x.shape, c0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalize(x): \n",
    "    if len(x.shape) == 3: x = x.mean(1)\n",
    "    return x / jnp.linalg.norm(x, axis=-1, keepdims=True)\n",
    "\n",
    "def encode_img(img,model_type=backbone.split('_')[0]):\n",
    "    if model_type == 'clip' or model_type == 'siglip':\n",
    "        zimg, _, _ = bv_model.apply({\"params\":params}, img)\n",
    "    elif model_type == 'cappa' or model_type == 'coca':\n",
    "        encoded, _ = bv_model.apply({\"params\":params}, img, return_enc_features=True, method = 'encode')\n",
    "        return encoded\n",
    "    elif model_type.split('-')[0] == 'gemma2b':\n",
    "        encoded, _ = bv_model.apply({\"params\":params}, img, method='embed_image')\n",
    "        zimg = mean_normalize(encoded)\n",
    "    return zimg\n",
    "\n",
    "def encode_txt(txt,model_type=backbone.split('_')[0]):\n",
    "    if model_type == 'clip' or model_type == 'siglip':\n",
    "        _, ztxt, _ = bv_model.apply({\"params\":params}, None, text=txt['labels'])\n",
    "    elif model_type.split('-')[0] == 'gemma2b':\n",
    "        assert isinstance(txt, dict), f\"txt must be a dict, got {type(txt)}\"\n",
    "        ztxt, _ = bv_model.apply({\"params\":params}, None, txt['text'][:,:-1], txt['mask_ar'][:,:-1],is_blind=True)[1]['llm/pre_logits']\n",
    "        ztxt = mean_normalize(ztxt)\n",
    "    return ztxt\n",
    "\n",
    "def bv_perplexity(img,txt):\n",
    "    print(\"Encoding image\")\n",
    "    encoded = encode_img(img)\n",
    "    print(\"Decoding image to text\")\n",
    "    out = bv_model.apply({\"params\": params}, encoded, txt['labels'], method='decode')\n",
    "    if isinstance(out, tuple):\n",
    "        print(\"Computing similarity\")\n",
    "        logits, ztxt = out\n",
    "        zimg = mean_normalize(encoded)\n",
    "        ztxt = mean_normalize(ztxt)\n",
    "        sim = jnp.sum(zimg * ztxt, axis=-1)\n",
    "    else:\n",
    "        logits = out\n",
    "        sim = None\n",
    "    weights = jnp.where(txt['labels'] != 0, 1, 0).astype(jnp.float32)\n",
    "    print(\"Computing perplexities\")\n",
    "    losses = u.weighted_softmax_xent(\n",
    "        logits=logits, labels=txt['labels'],\n",
    "        weights=weights, label_smoothing=0.0,\n",
    "        reduction=False, normalize=False)\n",
    "    return {\n",
    "        \"perplexity\": losses * -1,\n",
    "        \"similarity\": sim,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing logits for clip model\n",
      "Computing image embeddings\n",
      "Computing text embeddings\n",
      "Computing similarities\n"
     ]
    }
   ],
   "source": [
    "model_type = backbone.split('_')[0]\n",
    "print(f\"Computing logits for {model_type} model\")\n",
    "if model_type in ['clip','siglip']:\n",
    "    print(\"Computing image embeddings\")\n",
    "    zi0, zi1 = encode_img(i0), encode_img(i1)\n",
    "    print(\"Computing text embeddings\")\n",
    "    zc0, zc1 = encode_txt(c0), encode_txt(c1)\n",
    "    print(\"Computing similarities\")\n",
    "    i0_c0 = jnp.sum(zi0 * zc0, axis=-1)\n",
    "    i0_c1 = jnp.sum(zi0 * zc1, axis=-1)\n",
    "    i1_c0 = jnp.sum(zi1 * zc0, axis=-1)\n",
    "    i1_c1 = jnp.sum(zi1 * zc1, axis=-1)\n",
    "\n",
    "elif model_type in ['cappa','coca']:\n",
    "    i0_c0 = bv_perplexity(i0,c0)\n",
    "    i0_c1 = bv_perplexity(i0,c1)\n",
    "    i1_c0 = bv_perplexity(i1,c0)\n",
    "    i1_c1 = bv_perplexity(i1,c1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Winoground scores\n",
      "Text Score: 30.00\n",
      "Image Score: 11.00\n",
      "Both Score: 7.50\n"
     ]
    }
   ],
   "source": [
    "if isinstance(i0_c0,dict):\n",
    "    for k in i0_c0.keys():\n",
    "        print(f\"Computing scores from {k}\")\n",
    "        compute_winoground_scores(i0_c0[k],i0_c1[k],i1_c0[k],i1_c1[k])\n",
    "else:\n",
    "    compute_winoground_scores(i0_c0,i0_c1,i1_c0,i1_c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open_CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "# open_clip_ckpt = \"/home/austinwang/vit_b_16-laion400m_e32-55e67d44.pt\"\n",
    "open_clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained=\"datacomp_xl_s13b_b90k\")\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "i0 = torch.cat([preprocess(examples[i]['image_0'].convert('RGB')).unsqueeze(0) for i in range(len(examples))])\n",
    "i1 = torch.cat([preprocess(examples[i]['image_1'].convert('RGB')).unsqueeze(0) for i in range(len(examples))])\n",
    "c0 = tokenizer(examples['caption_0'])\n",
    "c1 = tokenizer(examples['caption_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding images\n",
      "Encoding text\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Encoding images\")\n",
    "    zi0 = open_clip_model.encode_image(i0)\n",
    "    zi1 = open_clip_model.encode_image(i1)\n",
    "    print(\"Encoding text\")\n",
    "    zc0 = open_clip_model.encode_text(c0)\n",
    "    zc1 = open_clip_model.encode_text(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "i0_c0 = (zi0 * zc0).sum(-1).cpu().numpy()\n",
    "i0_c1 = (zi0 * zc1).sum(-1).cpu().numpy()\n",
    "i1_c0 = (zi1 * zc0).sum(-1).cpu().numpy()\n",
    "i1_c1 = (zi1 * zc1).sum(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([65.69056 , 39.836163, 57.971386, 28.234322, 44.676838],\n",
       "       dtype=float32),\n",
       " array([65.88536 , 37.76516 , 60.367546, 30.57318 , 43.058273],\n",
       "       dtype=float32),\n",
       " array([72.750755, 53.282722, 62.647163, 45.93105 , 39.767345],\n",
       "       dtype=float32),\n",
       " array([71.42322 , 51.538418, 64.43231 , 53.727386, 37.324226],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i0_c0[:5],i0_c1[:5],i1_c0[:5],i1_c1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Winoground scores\n",
      "Text Score: 29.25\n",
      "Image Score: 7.75\n",
      "Both Score: 6.50\n"
     ]
    }
   ],
   "source": [
    "compute_winoground_scores(i0_c0,i0_c1,i1_c0,i1_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
