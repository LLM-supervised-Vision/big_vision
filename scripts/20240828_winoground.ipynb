{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/austinwang/austin_big_vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ~/austin_big_vision\n",
    "import jax\n",
    "import json\n",
    "import importlib\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "import jax.numpy as jnp\n",
    "import tensorflow as tf\n",
    "\n",
    "from datasets import load_dataset\n",
    "import big_vision.pp.builder as pp_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "backbone = 'gemma2b-half-0.1_b16-F_contrastive'\n",
    "backbone = 'clip_s9b_bs32k'\n",
    "\n",
    "# setup\n",
    "backbone_dict = {\n",
    "    'clip': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_bs16384_warm10k_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_07-23_1510',\n",
    "    'clip_map': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_autoregressive_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019',\n",
    "    'clip_s9b': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_autoregressive_s9b_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_08-04_0839',\n",
    "    'clip_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-09_0655',\n",
    "    'siglip': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_parallel_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019',\n",
    "    'siglip_v4-32': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_replication_pod_04-11_2247',\n",
    "    'siglip_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-04_0839',\n",
    "    'cappa': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/cappa_bs16384_s3B_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-27_2108',\n",
    "    # 'cappa_decoder-qknorm-T_warm0.02': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_warm0.02_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-15_2102',\n",
    "    'cappa_s9b': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_s9B_warm0.02_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-27_2108',\n",
    "    'cappa_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs32768_s9B_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_08-07_2217',\n",
    "    'coca_6lyr': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_bs16384_warm0.03_1.0co-2.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-30_1841',\n",
    "    'coca_unified': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_s3b_bs16384_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-19_0355',\n",
    "    'coca_1.0co_1.0ca_6lyr_qknorm-T_warm0.02': 'gs://us-central2-storage/tensorflow_datasets/ckpts/coca_replication_bs16384_warm0.02_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-10_2225',\n",
    "    'coca': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-12_2313',\n",
    "    'coca_6lyr_1.0co_1.0ca_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_s9b_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-14_1614',\n",
    "    'gemma2b-half-0.1_b16-F_contrastive': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-half-0.1_so400m-F_contrastive_bs16384_s3b_wd1e-4_08-21_1935',\n",
    "}\n",
    "backbone_path = backbone_dict[backbone]\n",
    "config_path = f'{backbone_path}/config.json'\n",
    "config = ml_collections.ConfigDict(json.load(tf.io.gfile.GFile(config_path, \"r\")))\n",
    "for m in config.get(\"pp_modules\", [\"ops_general\", \"ops_image\", \"ops_text\"]): importlib.import_module(f\"big_vision.pp.{m}\")\n",
    "\n",
    "# load model\n",
    "print(f\"Loading model\")\n",
    "model_cfg = config.model\n",
    "img_key = 'img' if 'image' in model_cfg or 'img' in model_cfg else 'encoder'\n",
    "model_mod = importlib.import_module(f\"big_vision.models.{config.model_name}\")\n",
    "bv_model = model_mod.Model(**model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-09_0655\n"
     ]
    }
   ],
   "source": [
    "# load ckpt weights\n",
    "print(f'Loading weights from {backbone_path}')\n",
    "rng = jax.random.PRNGKey(42)\n",
    "dummy_img = jnp.zeros([2, 224, 224, 3], jnp.float32)\n",
    "dummy_txt = jnp.zeros([2, 64], jnp.int32)\n",
    "dummy_mask_ar = jnp.zeros([2, 64], jnp.bool_) if 'llm' in model_cfg else None\n",
    "if dummy_mask_ar is not None:\n",
    "    init_params = jax.jit(bv_model.init, backend=\"cpu\")(rng, dummy_img, dummy_txt,dummy_mask_ar)['params']\n",
    "else:\n",
    "    init_params = jax.jit(bv_model.init, backend=\"cpu\")(rng, dummy_img, dummy_txt)['params']\n",
    "\n",
    "img_load_kw = {'dont_load': ('.*_ln/scale','head/kernel', 'head/bias')}\n",
    "ckpt_path = f'{backbone_path}/checkpoint.bv-{config.total_steps:09d}'\n",
    "params = model_mod.load(init_params, ckpt_path, model_cfg, img_load_kw)\n",
    "jax.tree_map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winoground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing: decode|resize(224)|flip_lr|value_range(-1,1)|tokenize(max_len=64, model=\"c4_en\", clip_bpe=False, eos=\"sticky\", pad_value=1, inkey=\"caption\", outkey=\"labels\")|keep(\"image\", \"labels\")\n",
      "Preprocessing images\n",
      "Preprocessing text\n",
      "i0.shape: (400, 224, 224, 3), c0 shapes: {'labels': (400, 64)}\n"
     ]
    }
   ],
   "source": [
    "examples = load_dataset('facebook/winoground', token=\"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\",trust_remote_code=True)['test']\n",
    "print(f\"Applying preprocessing: {config.input.pp}\")\n",
    "pp_img_idx = config.input.pp.split('|').index('value_range(-1,1)')\n",
    "pp_list = config.input.pp.split('|')\n",
    "assert pp_list[0] == 'decode'\n",
    "pp_img = pp_builder.get_preprocess_fn('|'.join(pp_list[1:pp_img_idx+1]))\n",
    "pp_txt = pp_builder.get_preprocess_fn('|'.join(pp_list[pp_img_idx+1:]))\n",
    "print(\"Preprocessing images\")\n",
    "i0 = jnp.array([pp_img({\"image\": jnp.asarray(img.convert('RGB'))})['image'] for img in examples['image_0']])\n",
    "i1 = jnp.array([pp_img({\"image\": jnp.asarray(img.convert('RGB'))})['image'] for img in examples['image_1']])\n",
    "print(\"Preprocessing text\")\n",
    "c0_list, c0 = [pp_txt({\"caption\": txt}) for txt in examples['caption_0']], {}\n",
    "for k in c0_list[0].keys(): c0[k] = jnp.array([d[k] for d in c0_list])\n",
    "c1_list, c1 = [pp_txt({\"caption\": txt}) for txt in examples['caption_1']], {}\n",
    "for k in c1_list[0].keys(): c1[k] = jnp.array([d[k] for d in c1_list])\n",
    "print(f\"i0.shape: {i0.shape}, c0 shapes: {jax.tree.map(lambda x: x.shape, c0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalize(x): \n",
    "    x = x.mean(1)\n",
    "    return x / jnp.linalg.norm(x, axis=-1, keepdims=True)\n",
    "\n",
    "def bv_zimg(img):\n",
    "    if backbone.split('_')[0] == 'clip':\n",
    "        zimg, _, _ = bv_model.apply({\"params\":params}, img)\n",
    "    elif backbone.split('_')[0].split('-')[0] == 'gemma2b':\n",
    "        zimg, _ = bv_model.apply({\"params\":params}, img, method='embed_image')\n",
    "        zimg = mean_normalize(zimg)\n",
    "    return zimg\n",
    "\n",
    "def bv_ztxt(txt):\n",
    "    if backbone.split('_')[0] == 'clip':\n",
    "        _, ztxt, _ = bv_model.apply({\"params\":params}, None, text=txt['labels'])\n",
    "    elif backbone.split('_')[0].split('-')[0] == 'gemma2b':\n",
    "        assert isinstance(txt, dict), f\"txt must be a dict, got {type(txt)}\"\n",
    "        ztxt, _ = bv_model.apply({\"params\":params}, None, txt['text'][:,:-1], txt['mask_ar'][:,:-1],is_blind=True)[1]['llm/pre_logits']\n",
    "        ztxt = mean_normalize(ztxt)\n",
    "    return ztxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing image embeddings\n",
      "Computing text embeddings\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing image embeddings\")\n",
    "zi0, zi1 = bv_zimg(i0), bv_zimg(i1)\n",
    "\n",
    "print(\"Computing text embeddings\")\n",
    "zc0, zc1 = bv_ztxt(c0), bv_ztxt(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarities\n",
      "Computing Winoground scores\n",
      "Text Score: 13.25\n",
      "Image Score: 2.00\n",
      "Both Score: 1.25\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing similarities\")\n",
    "i0_c0 = jnp.sum(zi0 * zc0, axis=-1)\n",
    "i0_c1 = jnp.sum(zi0 * zc1, axis=-1)\n",
    "i1_c0 = jnp.sum(zi1 * zc0, axis=-1)\n",
    "i1_c1 = jnp.sum(zi1 * zc1, axis=-1)\n",
    "\n",
    "print(\"Computing Winoground scores\")\n",
    "# text scores: 1 if i0_c0 > i0_c1 and i1_c1 > i1_c0, 0 otherwise\n",
    "# image scores: 1 if i0_c0 > i1_c0 and i1_c1 > i0_c1, 0 otherwise\n",
    "text_scores = (i0_c0 > i0_c1) & (i1_c1 > i1_c0)\n",
    "image_scores = (i0_c0 > i1_c0) & (i1_c1 > i0_c1)\n",
    "both_scores = text_scores & image_scores\n",
    "def get_acc(scores): \n",
    "    # return eg.72.50%\n",
    "    s = scores.mean().item()*100\n",
    "    return f'{s:.2f}'\n",
    "print(\"Text Score:\", get_acc(text_scores))\n",
    "print(\"Image Score:\", get_acc(image_scores))\n",
    "print(\"Both Score:\", get_acc(both_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
