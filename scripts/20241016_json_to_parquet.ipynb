{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to Parquet: 98it [02:57,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /tmp/cambrian_dataset_10M.parquet uploaded to gs://us-central2-storage/tensorflow_datasets/tensorflow_datasets/cambrian_dataset/cambrian_dataset_10M.parquet\n",
      "Local Parquet file removed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "def json_to_parquet(json_file, parquet_file, chunk_size=100000):\n",
    "    # Function to read JSON in chunks\n",
    "    def read_json_chunks(file, chunk_size):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for i in range(0, len(data), chunk_size):\n",
    "                yield data[i:i + chunk_size]\n",
    "\n",
    "    # Initialize the Parquet writer\n",
    "    schema = None\n",
    "    writer = None\n",
    "\n",
    "    # Process the JSON file in chunks\n",
    "    for chunk in tqdm(read_json_chunks(json_file, chunk_size), desc=\"Converting to Parquet\"):\n",
    "        df = pd.DataFrame(chunk)\n",
    "        \n",
    "        # Convert 'id' column to string\n",
    "        if 'id' in df.columns:\n",
    "            df['id'] = df['id'].astype(str)\n",
    "        \n",
    "        # If 'image' column is missing, add it as an empty string column\n",
    "        if 'image' not in df.columns:\n",
    "            df['image'] = ''\n",
    "        \n",
    "        if schema is None:\n",
    "            # Create a schema based on the DataFrame\n",
    "            schema = pa.Schema.from_pandas(df)\n",
    "            writer = pq.ParquetWriter(parquet_file, schema)\n",
    "        \n",
    "        table = pa.Table.from_pandas(df, schema=schema)\n",
    "        writer.write_table(table)\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "def upload_to_gcs(local_file, gcs_path):\n",
    "    storage_client = storage.Client()\n",
    "    bucket_name = gcs_path.split('/')[2]\n",
    "    blob_name = '/'.join(gcs_path.split('/')[3:])\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    blob.upload_from_filename(local_file)\n",
    "    print(f\"File {local_file} uploaded to {gcs_path}\")\n",
    "\n",
    "# File paths\n",
    "json_file = \"/mnt/disks/storage/data/finetune_data/clean_9784k.json\"\n",
    "local_parquet_file = \"/tmp/cambrian_dataset_10M.parquet\"\n",
    "gcs_path = \"gs://us-central2-storage/tensorflow_datasets/tensorflow_datasets/cambrian_dataset/cambrian_dataset_10M.parquet\"\n",
    "\n",
    "# Convert JSON to Parquet\n",
    "json_to_parquet(json_file, local_parquet_file)\n",
    "\n",
    "# Upload to GCS\n",
    "upload_to_gcs(local_parquet_file, gcs_path)\n",
    "\n",
    "# Clean up local Parquet file\n",
    "os.remove(local_parquet_file)\n",
    "print(\"Local Parquet file removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(local_parquet_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
