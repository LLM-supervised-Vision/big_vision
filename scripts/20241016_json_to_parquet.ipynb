{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from google.cloud import storage\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def json_to_parquet_gcs(json_file, gcs_bucket, gcs_prefix, chunk_size=10000):\n",
    "    # Initialize GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(gcs_bucket)\n",
    "\n",
    "    # Function to read JSON in chunks\n",
    "    def read_json_chunks(file, chunk_size):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for i in range(0, len(data), chunk_size):\n",
    "                yield data[i:i + chunk_size]\n",
    "\n",
    "    # Process the JSON file in chunks\n",
    "    for i, chunk in enumerate(tqdm(read_json_chunks(json_file, chunk_size), desc=\"Converting to Parquet\")):\n",
    "        df = pd.DataFrame(chunk)\n",
    "        \n",
    "        # Convert 'id' column to string\n",
    "        if 'id' in df.columns:\n",
    "            df['id'] = df['id'].astype(str)\n",
    "        \n",
    "        # If 'image' column is missing, add it as an empty string column\n",
    "        if 'image' not in df.columns:\n",
    "            df['image'] = ''\n",
    "        \n",
    "        # Convert DataFrame to PyArrow Table\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        \n",
    "        # Write to Parquet in memory\n",
    "        buf = io.BytesIO()\n",
    "        pq.write_table(table, buf)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to GCS\n",
    "        blob_name = f\"{gcs_prefix}/cambrian_dataset_10M_part_{i:05d}.parquet\"\n",
    "        blob = bucket.blob(blob_name)\n",
    "        blob.upload_from_file(buf, content_type='application/octet-stream')\n",
    "        \n",
    "        print(f\"Uploaded {blob_name}\")\n",
    "\n",
    "# File paths and GCS details\n",
    "json_file = \"/mnt/disks/storage/data/finetune_data/clean_9784k.json\"\n",
    "gcs_bucket = \"us-central2-storage\"\n",
    "gcs_prefix = \"us-central2-storage/tensorflow_datasets/tensorflow_datasets/downloads/manual_cambrian_dataset\"\n",
    "\n",
    "# Convert JSON to Parquet and upload to GCS\n",
    "json_to_parquet_gcs(json_file, gcs_bucket, gcs_prefix)\n",
    "\n",
    "print(\"Conversion and upload complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample with ID 12378:\n",
      "id: 12378\n",
      "image: ai2d/ai2d/images/1310.png\n",
      "conversations: [{'from': 'human', 'value': '<image>\\nWhat phase is shown above'}\n",
      " {'from': 'gpt', 'value': 'photosynthesis'}]\n",
      "source: ai2d_15k.json\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "def read_sample_by_id(bucket_name, prefix, sample_id):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    \n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.parquet'):\n",
    "            data = blob.download_as_bytes()\n",
    "            table = pq.read_table(BytesIO(data))\n",
    "            df = table.to_pandas()\n",
    "            \n",
    "            # Ensure 'id' column is treated as string\n",
    "            df['id'] = df['id'].astype(str)\n",
    "            \n",
    "            sample = df[df['id'] == str(sample_id)]\n",
    "            if not sample.empty:\n",
    "                return sample.iloc[0].to_dict()\n",
    "    \n",
    "    return None  # Sample not found\n",
    "\n",
    "# Use the function\n",
    "bucket_name = \"us-central2-storage\"\n",
    "prefix = \"tensorflow_datasets/tensorflow_datasets/downloads/manual_cambrian_dataset\"\n",
    "sample_id = 12378  # Replace with the desired sample ID\n",
    "\n",
    "sample = read_sample_by_id(bucket_name, prefix, sample_id)\n",
    "\n",
    "if sample:\n",
    "    print(f\"Sample with ID {sample_id}:\")\n",
    "    for key, value in sample.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(f\"Sample with ID {sample_id} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
