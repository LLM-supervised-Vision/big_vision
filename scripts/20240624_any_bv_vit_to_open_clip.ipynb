{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/austinwang/austin_big_vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-18 13:13:02.177266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-18 13:13:02.199512: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-18 13:13:02.206412: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-18 13:13:03.148283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%cd ~/austin_big_vision\n",
    "import io\n",
    "import jax\n",
    "import torch\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import open_clip\n",
    "from big_vision import utils as u\n",
    "from big_vision.models.vit import scan_to_pyloop\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "save_bv_vit = False\n",
    "np_save_path = \"/home/austinwang/ckpts_npz/bv_siglip.npz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'total_steps', 'init_shapes', 'init_types', 'log_training_steps', 'ckpt_steps', 'model_name', 'model_load', 'model', 'loss_fn', 'optax_name', 'optax', 'mesh', 'sharding_strategy', 'lr', 'wd', 'schedule', 'grad_clip_norm', 'evals', 'wandb'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load json file from google cloud storage\n",
    "import json\n",
    "import tensorflow as tf\n",
    "gs_path = \"gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_parallel_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019/config.json\"\n",
    "\n",
    "with tf.io.gfile.GFile(gs_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# big_vision ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721308386.834536 3665871 gcs_resource.cc:109] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1721308386.842992 3667264 google_auth_provider.cc:180] Running on GCE, using service account 373177222751-compute@developer.gserviceaccount.com\n",
      "/tmp/ipykernel_3665871/93864302.py:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  jax.tree_map(jnp.shape, params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAPHead_0': {'LayerNorm_0': {'bias': (768,), 'scale': (768,)},\n",
       "  'MlpBlock_0': {'Dense_0': {'bias': (3072,), 'kernel': (768, 3072)},\n",
       "   'Dense_1': {'bias': (768,), 'kernel': (3072, 768)}},\n",
       "  'MultiHeadDotProductAttention_0': {'key': {'bias': (12, 64),\n",
       "    'kernel': (768, 12, 64)},\n",
       "   'out': {'bias': (768,), 'kernel': (12, 64, 768)},\n",
       "   'query': {'bias': (12, 64), 'kernel': (768, 12, 64)},\n",
       "   'value': {'bias': (12, 64), 'kernel': (768, 12, 64)}},\n",
       "  'probe': (1, 1, 768)},\n",
       " 'Transformer': {'encoder_norm': {'bias': (768,), 'scale': (768,)},\n",
       "  'encoderblock': {'LayerNorm_0': {'bias': (12, 768), 'scale': (12, 768)},\n",
       "   'LayerNorm_1': {'bias': (12, 768), 'scale': (12, 768)},\n",
       "   'MlpBlock_0': {'Dense_0': {'bias': (12, 3072), 'kernel': (12, 768, 3072)},\n",
       "    'Dense_1': {'bias': (12, 768), 'kernel': (12, 3072, 768)}},\n",
       "   'MultiHeadDotProductAttention_0': {'key': {'bias': (12, 12, 64),\n",
       "     'kernel': (12, 768, 12, 64)},\n",
       "    'out': {'bias': (12, 768), 'kernel': (12, 12, 64, 768)},\n",
       "    'query': {'bias': (12, 12, 64), 'kernel': (12, 768, 12, 64)},\n",
       "    'value': {'bias': (12, 12, 64), 'kernel': (12, 768, 12, 64)}}}},\n",
       " 'embedding': {'bias': (768,), 'kernel': (16, 16, 3, 768)},\n",
       " 'pos_embedding': (1, 196, 768)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import big_vision.models.vit as model_mod\n",
    "model_cfg = ml_collections.ConfigDict()\n",
    "model_cfg.num_classes = None\n",
    "model_cfg.patch_size = (16, 16)\n",
    "model_cfg.width = 768\n",
    "model_cfg.depth = 12\n",
    "model_cfg.mlp_dim = None  # Defaults to 4x input dim\n",
    "model_cfg.num_heads = 12\n",
    "model_cfg.posemb = \"learn\"  # Can also be \"sincos2d\"\n",
    "model_cfg.rep_size = False\n",
    "model_cfg.dropout = 0.0\n",
    "model_cfg.pool_type = \"map\"  # Can also be \"map\" or \"tok\"\n",
    "model_cfg.head_zeroinit = True\n",
    "model_cfg.mask = None\n",
    "model_cfg.normalize_qk = False\n",
    "model_cfg.scan = True\n",
    "model_cfg.remat_policy = \"nothing_saveable\"\n",
    "model_cfg.dtype_mm = \"bfloat16\"\n",
    "\n",
    "bv_model = model_mod.Model(**model_cfg)\n",
    "\n",
    "# cappa_ckpt = 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_warm0.02_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-15_2102/checkpoint.bv-000183105'\n",
    "# cappa_path = cappa_ckpt+':encoder'\n",
    "siglip_ckpt = 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_replication_pod_04-11_2247/checkpoint.bv-000183105'\n",
    "siglip_path = siglip_ckpt+':img'\n",
    "\n",
    "init_params = None\n",
    "params = model_mod.load(init_params, siglip_path, model_cfg)\n",
    "jax.tree_map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_bv_vit:\n",
    "    # save big_vision ckpt as npz file\n",
    "    ckpt = {'params': {'img': scan_to_pyloop(params)}}\n",
    "    names_and_vals, _ = u.tree_flatten_with_names(ckpt)\n",
    "    io_buffer = io.BytesIO()\n",
    "    np.savez(io_buffer, **{k: v for k, v in names_and_vals})\n",
    "    with open(np_save_path, \"wb\") as f:\n",
    "        f.write(io_buffer.getvalue())\n",
    "\n",
    "    # load npz file\n",
    "    big_vision_vit = np.load(np_save_path)\n",
    "    for key in big_vision_vit.keys():\n",
    "        print(key, big_vision_vit[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def load_any_bv_vit(model, checkpoint_path, strict=True):\n",
    "    print(f\"HELLO! I changed the load_checkpoint function!!!!\")\n",
    "    from timm.layers import resample_patch_embed, resample_abs_pos_embed\n",
    "\n",
    "    def _n2p(w, t=True):\n",
    "        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n",
    "            w = w.flatten()\n",
    "        if t:\n",
    "            if w.ndim == 4:\n",
    "                w = w.transpose([3, 2, 0, 1])\n",
    "            elif w.ndim == 3:\n",
    "                w = w.transpose([2, 0, 1])\n",
    "            elif w.ndim == 2:\n",
    "                w = w.transpose([1, 0])\n",
    "        return torch.from_numpy(w)\n",
    "\n",
    "    w = np.load(checkpoint_path)\n",
    "    interpolation = 'bilinear'\n",
    "    antialias = False\n",
    "    \n",
    "    module = model.visual.trunk\n",
    "    prefix = 'params/img/'\n",
    "    embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n",
    "    if embed_conv_w.shape[-2:] != module.patch_embed.proj.weight.shape[-2:]:\n",
    "        embed_conv_w = resample_patch_embed(\n",
    "            embed_conv_w,\n",
    "            module.patch_embed.proj.weight.shape[-2:],\n",
    "            interpolation=interpolation,\n",
    "            antialias=antialias,\n",
    "            verbose=True,\n",
    "        )\n",
    "    module.patch_embed.proj.weight.copy_(embed_conv_w)\n",
    "    module.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n",
    "\n",
    "    if module.cls_token is not None:\n",
    "        module.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n",
    "\n",
    "    pos_embed_w = _n2p(w[f'{prefix}pos_embedding'], t=False)\n",
    "    if pos_embed_w.shape != module.pos_embed.shape:\n",
    "        assert False, f'{pos_embed_w.shape}, {module.pos_embed.shape}'\n",
    "        num_prefix_tokens = 0 if getattr(module, 'no_embed_class', False) else getattr(module, 'num_prefix_tokens', 1)\n",
    "        pos_embed_w = resample_abs_pos_embed(  # resize pos embedding when different size from pretrained weights\n",
    "            pos_embed_w,\n",
    "            new_size=module.patch_embed.grid_size,\n",
    "            num_prefix_tokens=num_prefix_tokens,\n",
    "            interpolation=interpolation,\n",
    "            antialias=antialias,\n",
    "            verbose=True,\n",
    "        )\n",
    "    module.pos_embed.copy_(pos_embed_w)\n",
    "\n",
    "    mha_sub, b_sub, ln1_sub = (0, 0, 1)\n",
    "    for i, block in enumerate(module.blocks.children()):\n",
    "        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n",
    "        mha_prefix = block_prefix + f'MultiHeadDotProductAttention_{mha_sub}/'\n",
    "        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        block.attn.qkv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n",
    "        block.attn.qkv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n",
    "        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_{b_sub}/Dense_{r}/kernel']))\n",
    "            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_{b_sub}/Dense_{r}/bias']))\n",
    "        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_{ln1_sub}/scale']))\n",
    "        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_{ln1_sub}/bias']))\n",
    "\n",
    "    module.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n",
    "    module.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n",
    "\n",
    "    if module.attn_pool is not None:\n",
    "        block_prefix = f'{prefix}MAPHead_0/'\n",
    "        mha_prefix = block_prefix + f'MultiHeadDotProductAttention_0/'\n",
    "        module.attn_pool.latent.copy_(_n2p(w[f'{block_prefix}probe'], t=False))\n",
    "        module.attn_pool.q.weight.copy_(_n2p(w[f'{mha_prefix}query/kernel'], t=False).flatten(1).T)\n",
    "        module.attn_pool.q.bias.copy_(_n2p(w[f'{mha_prefix}query/bias'], t=False).reshape(-1))\n",
    "        module.attn_pool.kv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('key', 'value')]))\n",
    "        module.attn_pool.kv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('key', 'value')]))\n",
    "        module.attn_pool.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        module.attn_pool.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        module.attn_pool.norm.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        module.attn_pool.norm.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(module.attn_pool.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_{r}/kernel']))\n",
    "            getattr(module.attn_pool.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_{r}/bias']))\n",
    "\n",
    "open_clip.convert.load_big_vision_weights = load_any_bv_vit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! I changed the load_checkpoint function!!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomTextCLIP(\n",
       "  (visual): TimmModel(\n",
       "    (trunk): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn_pool): AttentionPoolLatent(\n",
       "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (kv): Linear(in_features=768, out_features=1536, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (head): Sequential()\n",
       "  )\n",
       "  (text): TextTransformer(\n",
       "    (token_embedding): Embedding(32000, 768)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNormFp32((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNormFp32((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNormFp32((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (text_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = \"ViT-B-16-SigLIP\"\n",
    "open_clip_model, processor = open_clip.create_model_from_pretrained(\n",
    "    model_type,\n",
    "    pretrained=np_save_path,\n",
    "    precision='bf16',\n",
    ")\n",
    "open_clip_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# big_vision vs. open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Got unsupported ScalarType BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# initialize dummy image with same values\u001b[39;00m\n\u001b[1;32m      7\u001b[0m open_clip_dummy_img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mbfloat16()\n\u001b[0;32m----> 8\u001b[0m bv_dummy_img \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mopen_clip_dummy_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m bv_logits \u001b[38;5;241m=\u001b[39m bv_model\u001b[38;5;241m.\u001b[39mapply({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m:params}, bv_dummy_img)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m open_clip_logits \u001b[38;5;241m=\u001b[39m open_clip_model(open_clip_dummy_img)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Got unsupported ScalarType BFloat16"
     ]
    }
   ],
   "source": [
    "num_tests = 10\n",
    "\n",
    "similarity_list = []\n",
    "for i in range(num_tests):\n",
    "    print(f\"test {i}\")\n",
    "    # initialize dummy image with same values\n",
    "    open_clip_dummy_img = torch.randn(1, 3, 224, 224).bfloat16()\n",
    "    bv_dummy_img = jnp.array(open_clip_dummy_img.permute(0, 2, 3, 1).numpy())\n",
    "    bv_logits = bv_model.apply({\"params\":params}, bv_dummy_img)[0][0]\n",
    "    open_clip_logits = open_clip_model(open_clip_dummy_img)[0][0]\n",
    "    # l2 normalization\n",
    "    bv_logits = bv_logits / jnp.linalg.norm(bv_logits)\n",
    "    open_clip_logits = open_clip_logits / torch.norm(open_clip_logits)\n",
    "    # cosine similarity\n",
    "    similarity = jnp.dot(bv_logits, open_clip_logits.detach().numpy())\n",
    "    similarity_list.append(similarity)\n",
    "sum(similarity_list) / num_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
