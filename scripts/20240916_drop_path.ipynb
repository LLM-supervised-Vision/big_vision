{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine: layer_id as carry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable shapes: {'params': {'ScanCheckpointResidualMLPBlock_0': {'Dense_0': {'bias': (12, 768), 'kernel': (12, 768, 768)}}}}\n",
      "layer_id: 12\n",
      "y shape: (12, 32, 768)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "batch_size = 32\n",
    "feature_dim = 768\n",
    "layers = 12\n",
    "rng = jax.random.key(42)\n",
    "x = jnp.ones((batch_size, feature_dim))\n",
    "\n",
    "class ResidualMLPBlock(nn.Module):\n",
    "  feature_dim: int = 3\n",
    "  @nn.compact\n",
    "  def __call__(self, layer_id, x):\n",
    "    h = nn.Dense(features=self.feature_dim)(x)\n",
    "    h = nn.relu(h)\n",
    "    o = x+h\n",
    "    return layer_id+1, o\n",
    "    # out = {}\n",
    "    # out['layer_id'] = layer_id\n",
    "    # out['o'] = o\n",
    "    # return layer_id+1, o, out\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "  n_layers: int = 4\n",
    "  feature_dim: int = 3\n",
    "  remat_policy: str = \"nothing_saveable\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "\n",
    "    remat_block = nn.remat(\n",
    "      ResidualMLPBlock, \n",
    "      prevent_cse=False,\n",
    "      policy=getattr(jax.checkpoint_policies, self.remat_policy, None),\n",
    "    )\n",
    "\n",
    "    scan_block = nn.scan(\n",
    "      remat_block, \n",
    "      variable_axes={'params': 0},\n",
    "      split_rngs={'params': True},\n",
    "      in_axes=nn.broadcast,\n",
    "      length=self.n_layers)(\n",
    "        feature_dim=self.feature_dim,)\n",
    "\n",
    "    init_layer_id = 0\n",
    "    final_layer_id, y = scan_block(init_layer_id, x)\n",
    "    return final_layer_id, y\n",
    "  \n",
    "\n",
    "model = ResidualMLP(n_layers=layers, feature_dim=feature_dim)\n",
    "variables = model.init(rng, x)\n",
    "layer_id, y = model.apply(variables, x)\n",
    "print(f\"variable shapes: {jax.tree.map(lambda x: x.shape, variables)}\")\n",
    "print(f\"layer_id: {layer_id}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mine: x and layer id as carry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable shapes: {'params': {'ScanCheckpointResidualMLPBlock_0': {'Dense_0': {'bias': (12, 768), 'kernel': (12, 768, 768)}}}}\n",
      "layer_id: 12\n",
      "y shape: (32, 768)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "batch_size = 32\n",
    "feature_dim = 768\n",
    "layers = 12\n",
    "rng = jax.random.key(42)\n",
    "x = jnp.ones((batch_size, feature_dim))\n",
    "\n",
    "class ResidualMLPBlock(nn.Module):\n",
    "  feature_dim: int = 3\n",
    "  @nn.compact\n",
    "  def __call__(self, carry, _):\n",
    "    layer_id, x = carry\n",
    "    h = nn.Dense(features=self.feature_dim)(x)\n",
    "    h = nn.relu(h)\n",
    "    o = x+h\n",
    "    carry = layer_id+1, o\n",
    "    return carry, None\n",
    "    # out = {}\n",
    "    # out['layer_id'] = layer_id\n",
    "    # out['o'] = o\n",
    "    # return layer_id+1, o, out\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "  n_layers: int = 4\n",
    "  feature_dim: int = 3\n",
    "  remat_policy: str = \"nothing_saveable\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "\n",
    "    remat_block = nn.remat(\n",
    "      ResidualMLPBlock, \n",
    "      prevent_cse=False,\n",
    "      policy=getattr(jax.checkpoint_policies, self.remat_policy, None),\n",
    "    )\n",
    "\n",
    "    scan_block = nn.scan(\n",
    "      remat_block, \n",
    "      variable_axes={'params': 0},\n",
    "      split_rngs={'params': True},\n",
    "      in_axes=nn.broadcast,\n",
    "      length=self.n_layers)(\n",
    "        feature_dim=self.feature_dim,)\n",
    "\n",
    "    init_layer_id = 0\n",
    "    carry = init_layer_id, x\n",
    "    final_carry, _ = scan_block(carry, None)\n",
    "    final_layer_id, y = final_carry\n",
    "    return final_layer_id, y\n",
    "  \n",
    "\n",
    "model = ResidualMLP(n_layers=layers, feature_dim=feature_dim)\n",
    "variables = model.init(rng, x)\n",
    "layer_id, y = model.apply(variables, x)\n",
    "print(f\"variable shapes: {jax.tree.map(lambda x: x.shape, variables)}\")\n",
    "print(f\"layer_id: {layer_id}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/flax/linen/transforms.py\u001b[0m(420)\u001b[0;36mwrapped_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    418 \u001b[0;31m  \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    419 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 420 \u001b[0;31m    \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    421 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    422 \u001b[0;31m    \u001b[0;31m# make a scope-function to transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute '_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m model \u001b[38;5;241m=\u001b[39m ResidualMLP(n_layers\u001b[38;5;241m=\u001b[39mlayers, feature_dim\u001b[38;5;241m=\u001b[39mfeature_dim)\n\u001b[1;32m     52\u001b[0m rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m layer_id, y, outs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, x)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable shapes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjax\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m\u001b[38;5;250m \u001b[39mx:\u001b[38;5;250m \u001b[39mx\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;250m \u001b[39mvariables)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 43\u001b[0m, in \u001b[0;36mResidualMLP.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m ScanMLP \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m     35\u001b[0m     scan_fn,\n\u001b[1;32m     36\u001b[0m     variable_axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m init_layer_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 43\u001b[0m final_layer_id, (y, outs) \u001b[38;5;241m=\u001b[39m \u001b[43mScanMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_layer_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_layer_id, y, outs\n",
      "File \u001b[0;32m/mnt/vlm-pd/miniconda3/envs/vlm/lib/python3.10/site-packages/flax/linen/transforms.py:420\u001b[0m, in \u001b[0;36mdecorator_lift_transform.<locals>.wrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(prewrapped_fns[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    419\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m; pdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[0;32m--> 420\u001b[0m   state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241m.\u001b[39mexport()\n\u001b[1;32m    422\u001b[0m   \u001b[38;5;66;03m# make a scope-function to transform\u001b[39;00m\n\u001b[1;32m    423\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcore_fn\u001b[39m(prewrapped_fn, class_fn, scopes, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute '_state'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "batch_size = 32\n",
    "feature_dim = 768\n",
    "layers = 12\n",
    "rng = jax.random.key(42)\n",
    "x = jnp.ones((batch_size, feature_dim))\n",
    "\n",
    "class ResidualMLPBlock(nn.Module):\n",
    "    feature_dim: int = 3\n",
    "    @nn.compact\n",
    "    def __call__(self, layer_id, x):\n",
    "        h = nn.Dense(features=self.feature_dim)(x)\n",
    "        h = nn.relu(h)\n",
    "        o = x + h\n",
    "        out = {}\n",
    "        out['layer_id'] = layer_id\n",
    "        out['o'] = o\n",
    "        return layer_id + 1, o, out\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    n_layers: int = 4\n",
    "    feature_dim: int = 3\n",
    "    remat_policy: str = \"nothing_saveable\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        block = nn.remat(\n",
    "            ResidualMLPBlock, \n",
    "            prevent_cse=False,\n",
    "            policy=getattr(jax.checkpoint_policies, self.remat_policy, None),\n",
    "        )(feature_dim=self.feature_dim)\n",
    "\n",
    "        def scan_fn(layer_id, x):\n",
    "            next_layer_id, o, out = block(layer_id, x)\n",
    "            return next_layer_id, (o, out)\n",
    "\n",
    "        ScanMLP = nn.scan(\n",
    "            scan_fn,\n",
    "            variable_axes={'params': 0},\n",
    "            split_rngs={'params': True},\n",
    "            in_axes=nn.broadcast,\n",
    "            length=self.n_layers\n",
    "        )\n",
    "\n",
    "        init_layer_id = 0\n",
    "        final_layer_id, (y, outs) = ScanMLP(init_layer_id, x)\n",
    "        return final_layer_id, y, outs\n",
    "\n",
    "model = ResidualMLP(n_layers=layers, feature_dim=feature_dim)\n",
    "variables = model.init(rng, x)\n",
    "layer_id, y, outs = model.apply(variables, x)\n",
    "\n",
    "print(f\"variable shapes: {jax.tree_map(lambda x: x.shape, variables)}\")\n",
    "print(f\"layer_id: {layer_id}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"outs structure: {jax.tree_map(lambda x: x.shape, outs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable shapes: {'params': {'ScanCheckpointResidualMLPBlock_0': {'Dense_0': {'bias': (12, 768), 'kernel': (12, 768, 768)}}}}\n",
      "y shape: (32, 768)\n",
      "scan_out shapes: {'o': (12, 32, 768)}\n"
     ]
    }
   ],
   "source": [
    "class ResidualMLPBlock(nn.Module):\n",
    "  feature_dim: int = 3\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    out = {}\n",
    "    h = nn.Dense(features=self.feature_dim)(x)\n",
    "    h = nn.relu(h)\n",
    "    o = out['o'] = x+h\n",
    "    return o, out\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "  n_layers: int = 4\n",
    "  feature_dim: int = 3\n",
    "  remat_policy: str = \"nothing_saveable\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    block = nn.remat(\n",
    "      ResidualMLPBlock, \n",
    "      prevent_cse=False,\n",
    "      static_argnums=(2,),\n",
    "      policy=getattr(jax.checkpoint_policies, self.remat_policy, None),)\n",
    "    \n",
    "    ScanMLP = nn.scan(\n",
    "      block, \n",
    "      variable_axes={'params': 0},\n",
    "      split_rngs={'params': True},\n",
    "      in_axes=nn.broadcast,\n",
    "      length=self.n_layers)\n",
    "    \n",
    "    x, scan_out = ScanMLP(\n",
    "      feature_dim=self.feature_dim\n",
    "    )(x,deterministic)\n",
    "    return x, scan_out\n",
    "\n",
    "\n",
    "model = ResidualMLP(n_layers=layers, feature_dim=feature_dim)\n",
    "variables = model.init(rng, x)\n",
    "y, scan_out = model.apply(variables, x, True)\n",
    "print(f\"variable shapes: {jax.tree.map(lambda x: x.shape, variables)}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"scan_out shapes: {jax.tree.map(lambda x: x.shape, scan_out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
