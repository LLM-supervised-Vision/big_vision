{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gavin/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/gavin/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gavin/big_vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 23:08:28.672465: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730243308.694856   19060 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730243308.701758   19060 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/home/gavin/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd ~/big_vision\n",
    "import io\n",
    "import os\n",
    "import jax\n",
    "import json\n",
    "import torch\n",
    "import importlib\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "import jax.numpy as jnp\n",
    "import tensorflow as tf\n",
    "\n",
    "import open_clip\n",
    "import big_vision.utils as u\n",
    "import big_vision.models.vit as model_mod\n",
    "from big_vision.models.vit import scan_to_pyloop\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nest-asyncio in /home/gavin/.local/lib/python3.10/site-packages (1.6.0)\n",
      "\u001b[33mWARNING: Error parsing dependencies of distro-info: Invalid version: '1.1build1'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of python-debian: Invalid version: '0.1.43ubuntu1'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nest-asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "save_bv_vit = True\n",
    "backbone = 'cappa_s9b_bs32k'\n",
    "\n",
    "# setup\n",
    "backbone_dict = {\n",
    "    'clip': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_bs16384_warm10k_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_07-23_1510',\n",
    "    'clip_map': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_autoregressive_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019',\n",
    "    'clip_s9b': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_autoregressive_s9b_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_08-04_0839',\n",
    "    'clip_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/clip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-09_0655',\n",
    "    'siglip': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_parallel_bs16384_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_12lyr_06-24_2019',\n",
    "    'siglip_v4-32': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_replication_pod_04-11_2247',\n",
    "    'siglip_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/siglip_s9b_bs32k_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_08-04_0839',\n",
    "    'cappa': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/cappa_bs16384_s3B_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-27_2108',\n",
    "    # 'cappa_decoder-qknorm-T_warm0.02': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_warm0.02_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-15_2102',\n",
    "    'cappa_s9b': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs16384_s9B_warm0.02_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-27_2108',\n",
    "    'cappa_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/cappa_bs32768_s9B_warm0.03_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_08-07_2217',\n",
    "    'coca_6lyr': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_bs16384_warm0.03_1.0co-2.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_06-30_1841',\n",
    "    'coca_unified': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_s3b_bs16384_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-19_0355',\n",
    "    'coca_1.0co_1.0ca_6lyr_qknorm-T_warm0.02': 'gs://us-central2-storage/tensorflow_datasets/ckpts/coca_replication_bs16384_warm0.02_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_b2-0.95_6lyr_06-10_2225',\n",
    "    'coca': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-12_2313',\n",
    "    'coca_6lyr_1.0co_1.0ca_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_replication_s9b_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-14_1614',\n",
    "    'gemma2b-half-0.1_b16-F_contrastive': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-half-0.1_so400m-F_contrastive_bs16384_s3b_wd1e-4_08-21_1935',\n",
    "    'gemma2b-contrastive_9+9_llm_gap_0.01': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-partial_frozen99-0.01-gap_b16-F_contrastive_bs16k_s3b_lr1e-3_wd1e-4_bf16_09-01_0446',\n",
    "    'gemma2b-contrastive_ffn_llm_1.0': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-adapter_ffn-1.0-drop0.0-vocab256128-projF_b16-beitF-qknormF_contrastive_dpr0.0_bs16k_s3b_lr1e-3_wd1e-4_bf16_10-08_1756',\n",
    "    'gemma2b-full-0.1_b16-F_generative': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-full-0.1_so400m-F_generative_bs16384_s3b_wd1e-4_08-25_0445',\n",
    "    'gemma2b-12+6-0.01_b16_generative': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-partial_frozen12-0.01-drop0.0-none_b16-beitF_contrastive_bs16k_s3b_lr1e-3_wd1e-4_bf16_09-08_0448',\n",
    "    'gemma2b_9+9_llm_0.02_b16_qknormT_generative': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-partial_frozen99-0.02-drop0.0-none_b16-beitF-qknormT_generative_bs16k_s3b_lr1e-3_wd1e-4_fp32_10-02_1429',\n",
    "    'coca_6lyr_s9b_bs32k': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_unified_s9b_bs32768_warm0.03_1.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_08-20_0246',\n",
    "    'coca_unified_1.0co_0.0ca': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_unified_s3b_bs16384_warm0.03_1.0co-0.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_09-05_0535',\n",
    "    'coca_unified_0.0co_1.0ca': 'gs://us-central2-storage/tensorflow_datasets/vit-b-16_3b_pretraining/coca_unified_s3b_bs16384_warm0.03_0.0co-1.0ca_lr1e-3_wd1e-4_bf16_qknorm-F_b2-0.95_6lyr_scan-F_fsdp-F_09-05_0603',\n",
    "    'clip_FT_50M_recapF': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/clip_s3b_bs16k_lr1e-5_wd0.0_bs16k_epoch5_warm0.03_recapFalse_50M_10-02_2028',\n",
    "    'clip_FT_50M_recapT': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/clip_s3b_bs16k_lr1e-5_wd0.0_bs16k_epoch5_warm0.03_recapTrue_50M_10-02_2028',\n",
    "    'clip+gemma_FT_recapF': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/clip_s3b_bs16k+gemma2b_lr1e-5_wd0.0_bs16k_epoch5_warm0.03_recapFalse_10-03_0440',\n",
    "    'clip+gemma_FT_recapT': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/clip_s3b_bs16k+gemma2b_lr1e-5_wd0.0_bs16k_epoch5_warm0.03_recapTrue_10-03_0440',\n",
    "    'gemma-supervised_FT_recapF': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/gemma-supervised_s3b_bs16k_lr1e-5_wd0.0_bs16k_epoch5_warm0.03_recapFalse_10-02_1727',\n",
    "    'gemma-supervised_FT_recapT': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/gemma-supervised_s3b_bs16k_lr1e-5_wd0.0_bs16k_epoch5_warm0.03_recapTrue_10-02_1437',\n",
    "    'gemma-supervised_PT_recapF': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-partial_frozen99-0.01-drop0.0-vocab256128-gap-projT_b16-beitF-qknormF_contrastive_datacomp-recapF-50M-epoch10_dpr0.0_bs16k_s3b_lr1e-3_wd1e-4_bf16_10-09_2043',\n",
    "    'gemma-supervised_PT_recapT': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-partial_frozen99-0.01-drop0.0-vocab256128-gap-projT_b16-beitF-qknormF_contrastive_datacomp-recapT-50M-epoch10_dpr0.0_bs16k_s3b_lr1e-3_wd1e-4_bf16_10-09_2039',\n",
    "    'gemma-supervised_PT_org0.5': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-partial_frozen99-0.01-drop0.0-vocab256128-gap-projT_b16-beitF-qknormF_contrastive_datacomp-org_ratio0.5-50M-epoch10_dpr0.0_bs16k_s3b_lr1e-3_wd1e-4_bf16_10-11_0801',\n",
    "    'gemma-supervised-generative_PT_org_10M': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-partial_frozen99-0.01-drop0.0-vocab256128-gap-projT-txtlen128_b16-beitF-qknormF_generative_datacomp-org_ratio1.0-10M-epoch10_dpr0.0_bs16k_s3b_lr1e-3_wd1e-4_bf16_10-17_1650',\n",
    "    'clip_PT_recapF': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/clip_PT_lr1e-3_wd1e-4_bs16k_epoch10_warm0.03_recapFalse_50M_10-10_0458',\n",
    "    'clip_PT_recapT': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/clip_PT_lr1e-3_wd1e-4_bs16k_epoch10_warm0.03_recapTrue_50M_10-10_1226',\n",
    "    'clip_PT_org0.5': 'gs://us-central2-storage/tensorflow_datasets/post-training-datacomp-recap/clip_PT_lr1e-3_wd1e-4_bs16k_epoch10_warm0.03_org-ratio0.5_50M_10-11_1516',\n",
    "    'llava_gemma_PT_cambrian10M-10epoch': 'gs://us-central2-storage/tensorflow_datasets/mllm_ckpts/paligemma/gemma2b-partial_frozen99-0.01-drop0.0-vocab256128-gap-projT-txtlen128_b16-beitF-qknormF_generative_datacomp-org_ratio1.0-10M-epoch10_dpr0.0_bs16k_s3b_lr1e-3_wd1e-4_bf16_10-29_1338',\n",
    "}\n",
    "backbone_path = backbone_dict[backbone]\n",
    "gcs_np_save_path = f'{backbone_path}/bv_{backbone}.npz'\n",
    "local_np_save_path = f'./ckpt_npz/bv_{backbone}.npz'\n",
    "if not os.path.exists('./ckpt_npz'): os.makedirs('./ckpt_npz')\n",
    "config_path = f'{backbone_path}/config.json'\n",
    "metrics_path = f'{backbone_path}/big_vision_metrics.txt'\n",
    "\n",
    "config = ml_collections.ConfigDict(json.load(tf.io.gfile.GFile(config_path, 'r')))\n",
    "bv_metrics = json.loads(tf.io.gfile.GFile(metrics_path, 'r').read().split('\\n')[-2])\n",
    "ckpt_path = f'{backbone_path}/checkpoint.bv-{config.total_steps:09d}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# big_vision ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1029 23:08:49.851347   20417 google_auth_provider.cc:181] Running on GCE, using service account 373177222751-compute@developer.gserviceaccount.com\n",
      "/tmp/ipykernel_19060/1131154012.py:22: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  jax.tree_map(jnp.dtype, img_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Transformer': {'encoder_norm': {'bias': dtype('float32'),\n",
       "   'scale': dtype('float32')},\n",
       "  'encoderblock': {'LayerNorm_0': {'bias': dtype('float32'),\n",
       "    'scale': dtype('float32')},\n",
       "   'LayerNorm_1': {'bias': dtype('float32'), 'scale': dtype('float32')},\n",
       "   'MlpBlock_0': {'Dense_0': {'bias': dtype('float32'),\n",
       "     'kernel': dtype('float32')},\n",
       "    'Dense_1': {'bias': dtype('float32'), 'kernel': dtype('float32')}},\n",
       "   'MultiHeadDotProductAttention_0': {'key': {'bias': dtype('float32'),\n",
       "     'kernel': dtype('float32')},\n",
       "    'out': {'bias': dtype('float32'), 'kernel': dtype('float32')},\n",
       "    'query': {'bias': dtype('float32'), 'kernel': dtype('float32')},\n",
       "    'value': {'bias': dtype('float32'), 'kernel': dtype('float32')}}}},\n",
       " 'embedding': {'bias': dtype('float32'), 'kernel': dtype('float32')},\n",
       " 'pos_embedding': dtype(bfloat16)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cfg = config.model\n",
    "img_key = 'img' if 'image' in model_cfg or 'img' in model_cfg else 'encoder'\n",
    "\n",
    "# load model\n",
    "model_mod = importlib.import_module(f\"big_vision.models.{config.model_name}\")\n",
    "bv_model = model_mod.Model(**model_cfg)\n",
    "\n",
    "# load ckpt weights\n",
    "rng = jax.random.PRNGKey(42)\n",
    "dummy_img = jnp.zeros([2, 224, 224, 3], jnp.float32)\n",
    "dummy_txt = jnp.zeros([2, 64], jnp.int32)\n",
    "dummy_mask_ar = jnp.zeros([2, 64], jnp.bool_) if 'llm' in model_cfg else None\n",
    "if dummy_mask_ar is not None:\n",
    "    init_params = jax.jit(bv_model.init, backend=\"cpu\")(rng, dummy_img, dummy_txt,dummy_mask_ar)['params']\n",
    "else:\n",
    "    init_params = jax.jit(bv_model.init, backend=\"cpu\")(rng, dummy_img, dummy_txt)['params']\n",
    "\n",
    "# img_load_kw = {'dont_load': ('.*_ln/scale','head/kernel', 'head/bias')}\n",
    "params = model_mod.load(init_params, ckpt_path, model_cfg) #, img_load_kw)\n",
    "img_params = params[img_key]\n",
    "# jax.tree_map(jnp.shape, img_params)\n",
    "jax.tree_map(jnp.dtype, img_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params/img/Transformer/encoder_norm/bias (768,) float32\n",
      "params/img/Transformer/encoder_norm/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_0/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_0/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_0/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_0/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_0/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_0/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_0/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_0/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_0/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_0/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_0/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_0/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_0/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_0/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_0/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_0/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_1/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_1/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_1/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_1/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_1/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_1/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_1/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_1/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_1/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_1/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_1/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_1/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_1/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_1/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_1/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_1/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_10/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_10/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_10/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_10/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_10/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_10/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_10/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_10/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_10/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_10/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_10/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_10/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_10/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_10/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_10/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_10/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_11/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_11/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_11/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_11/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_11/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_11/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_11/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_11/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_11/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_11/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_11/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_11/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_11/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_11/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_11/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_11/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_2/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_2/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_2/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_2/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_2/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_2/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_2/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_2/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_2/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_2/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_2/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_2/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_2/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_2/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_2/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_2/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_3/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_3/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_3/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_3/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_3/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_3/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_3/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_3/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_3/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_3/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_3/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_3/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_3/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_3/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_3/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_3/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_4/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_4/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_4/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_4/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_4/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_4/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_4/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_4/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_4/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_4/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_4/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_4/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_4/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_4/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_4/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_4/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_5/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_5/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_5/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_5/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_5/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_5/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_5/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_5/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_5/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_5/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_5/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_5/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_5/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_5/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_5/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_5/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_6/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_6/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_6/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_6/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_6/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_6/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_6/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_6/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_6/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_6/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_6/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_6/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_6/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_6/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_6/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_6/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_7/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_7/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_7/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_7/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_7/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_7/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_7/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_7/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_7/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_7/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_7/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_7/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_7/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_7/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_7/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_7/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_8/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_8/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_8/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_8/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_8/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_8/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_8/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_8/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_8/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_8/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_8/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_8/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_8/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_8/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_8/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_8/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_9/LayerNorm_0/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_9/LayerNorm_0/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_9/LayerNorm_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_9/LayerNorm_1/scale (768,) float32\n",
      "params/img/Transformer/encoderblock_9/MlpBlock_0/Dense_0/bias (3072,) float32\n",
      "params/img/Transformer/encoderblock_9/MlpBlock_0/Dense_0/kernel (768, 3072) float32\n",
      "params/img/Transformer/encoderblock_9/MlpBlock_0/Dense_1/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_9/MlpBlock_0/Dense_1/kernel (3072, 768) float32\n",
      "params/img/Transformer/encoderblock_9/MultiHeadDotProductAttention_0/key/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_9/MultiHeadDotProductAttention_0/key/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_9/MultiHeadDotProductAttention_0/out/bias (768,) float32\n",
      "params/img/Transformer/encoderblock_9/MultiHeadDotProductAttention_0/out/kernel (12, 64, 768) float32\n",
      "params/img/Transformer/encoderblock_9/MultiHeadDotProductAttention_0/query/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_9/MultiHeadDotProductAttention_0/query/kernel (768, 12, 64) float32\n",
      "params/img/Transformer/encoderblock_9/MultiHeadDotProductAttention_0/value/bias (12, 64) float32\n",
      "params/img/Transformer/encoderblock_9/MultiHeadDotProductAttention_0/value/kernel (768, 12, 64) float32\n",
      "params/img/embedding/bias (768,) float32\n",
      "params/img/embedding/kernel (16, 16, 3, 768) float32\n",
      "params/img/pos_embedding (1, 196, 768) float32\n"
     ]
    }
   ],
   "source": [
    "if save_bv_vit:\n",
    "    # save big_vision backbone vit as npz file\n",
    "    \n",
    "    ckpt = {'params': {'img': scan_to_pyloop(img_params) if 'encoderblock' in img_params['Transformer'].keys() else img_params}}\n",
    "    names_and_vals, _ = u.tree_flatten_with_names(ckpt)\n",
    "    names_and_vals = [(n, v.astype(np.float32)) for n, v in names_and_vals] # convert bf16 pos_emb to fp32 to avoid void np.dtype error\n",
    "    io_buffer = io.BytesIO()\n",
    "    np.savez(io_buffer, **{k: v for k, v in names_and_vals})\n",
    "\n",
    "    # save to gcs\n",
    "    with tf.io.gfile.GFile(gcs_np_save_path, \"wb\") as f: f.write(io_buffer.getvalue())\n",
    "    # save to local\n",
    "    with open(local_np_save_path, \"wb\") as f: f.write(io_buffer.getvalue())\n",
    "\n",
    "\n",
    "big_vision_vit = u.npload(gcs_np_save_path)\n",
    "# inspect keys, shapes, and dtypes\n",
    "for key in big_vision_vit.keys():\n",
    "    print(key, big_vision_vit[key].shape, big_vision_vit[key].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def load_any_bv_vit(model, checkpoint_path, strict=True):\n",
    "    print(f\"HELLO! I changed the load_checkpoint function!!!!\")\n",
    "    from timm.layers import resample_patch_embed, resample_abs_pos_embed\n",
    "\n",
    "    def _n2p(w, t=True):\n",
    "        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n",
    "            w = w.flatten()\n",
    "        if t:\n",
    "            if w.ndim == 4:\n",
    "                w = w.transpose([3, 2, 0, 1])\n",
    "            elif w.ndim == 3:\n",
    "                w = w.transpose([2, 0, 1])\n",
    "            elif w.ndim == 2:\n",
    "                w = w.transpose([1, 0])\n",
    "\n",
    "        # if w.dtype == np.dtype('|V2'):\n",
    "        #     w_float16 = w.view(np.float16)\n",
    "        #     w = w_float16.astype(np.float32)\n",
    "\n",
    "        return torch.from_numpy(w)\n",
    "\n",
    "    w = np.load(checkpoint_path)\n",
    "    interpolation = 'bilinear'\n",
    "    antialias = False\n",
    "    \n",
    "    module = model.visual.trunk\n",
    "    prefix = 'params/img/'\n",
    "    embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n",
    "    if embed_conv_w.shape[-2:] != module.patch_embed.proj.weight.shape[-2:]:\n",
    "        embed_conv_w = resample_patch_embed(\n",
    "            embed_conv_w,\n",
    "            module.patch_embed.proj.weight.shape[-2:],\n",
    "            interpolation=interpolation,\n",
    "            antialias=antialias,\n",
    "            verbose=True,\n",
    "        )\n",
    "    module.patch_embed.proj.weight.copy_(embed_conv_w)\n",
    "    module.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n",
    "\n",
    "    if module.cls_token is not None:\n",
    "        module.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n",
    "\n",
    "    pos_embed_w = _n2p(w[f'{prefix}pos_embedding'], t=False)\n",
    "    if pos_embed_w.shape != module.pos_embed.shape:\n",
    "        assert False, f'{pos_embed_w.shape}, {module.pos_embed.shape}'\n",
    "        num_prefix_tokens = 0 if getattr(module, 'no_embed_class', False) else getattr(module, 'num_prefix_tokens', 1)\n",
    "        pos_embed_w = resample_abs_pos_embed(  # resize pos embedding when different size from pretrained weights\n",
    "            pos_embed_w,\n",
    "            new_size=module.patch_embed.grid_size,\n",
    "            num_prefix_tokens=num_prefix_tokens,\n",
    "            interpolation=interpolation,\n",
    "            antialias=antialias,\n",
    "            verbose=True,\n",
    "        )\n",
    "    module.pos_embed.copy_(pos_embed_w)\n",
    "\n",
    "    mha_sub, b_sub, ln1_sub = (0, 0, 1)\n",
    "    for i, block in enumerate(module.blocks.children()):\n",
    "        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n",
    "        mha_prefix = block_prefix + f'MultiHeadDotProductAttention_{mha_sub}/'\n",
    "        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        block.attn.qkv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n",
    "        block.attn.qkv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n",
    "        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_{b_sub}/Dense_{r}/kernel']))\n",
    "            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_{b_sub}/Dense_{r}/bias']))\n",
    "        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_{ln1_sub}/scale']))\n",
    "        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_{ln1_sub}/bias']))\n",
    "\n",
    "    module_norm = module.norm if isinstance(module.norm,torch.nn.LayerNorm) else module.fc_norm\n",
    "    module_norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n",
    "    module_norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n",
    "\n",
    "    if module.attn_pool is not None:\n",
    "        block_prefix = f'{prefix}MAPHead_0/'\n",
    "        mha_prefix = block_prefix + f'MultiHeadDotProductAttention_0/'\n",
    "        module.attn_pool.latent.copy_(_n2p(w[f'{block_prefix}probe'], t=False))\n",
    "        module.attn_pool.q.weight.copy_(_n2p(w[f'{mha_prefix}query/kernel'], t=False).flatten(1).T)\n",
    "        module.attn_pool.q.bias.copy_(_n2p(w[f'{mha_prefix}query/bias'], t=False).reshape(-1))\n",
    "        module.attn_pool.kv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('key', 'value')]))\n",
    "        module.attn_pool.kv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('key', 'value')]))\n",
    "        module.attn_pool.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        module.attn_pool.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        module.attn_pool.norm.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        module.attn_pool.norm.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(module.attn_pool.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_{r}/kernel']))\n",
    "            getattr(module.attn_pool.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_0/Dense_{r}/bias']))\n",
    "\n",
    "open_clip.convert.load_big_vision_weights = load_any_bv_vit\n",
    "\n",
    "\n",
    "def qknorm_update(open_clip_model,np_save_path):\n",
    "    big_vision_vit = u.npload(np_save_path)\n",
    "    # check if qknorm is present\n",
    "    qknorm_present = any(['query_ln' in k for k in big_vision_vit.keys()])\n",
    "    print(f\"\\nqknorm_present: {qknorm_present}\")\n",
    "\n",
    "    if qknorm_present:\n",
    "        head_dim = open_clip_model.visual.trunk.blocks[0].attn.head_dim\n",
    "        for i, blk in enumerate(open_clip_model.visual.trunk.blocks):\n",
    "            # Replace query norm\n",
    "            blk.attn.q_norm = torch.nn.LayerNorm(head_dim, eps=1e-6)\n",
    "            blk.attn.q_norm.weight.data = torch.from_numpy(big_vision_vit[f'params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/query_ln/scale'])\n",
    "            blk.attn.q_norm.bias.data = torch.zeros_like(blk.attn.q_norm.bias)  # Set bias to zero as it's not present in the checkpoint\n",
    "            # Replace key norm\n",
    "            blk.attn.k_norm = torch.nn.LayerNorm(head_dim, eps=1e-6)\n",
    "            blk.attn.k_norm.weight.data = torch.from_numpy(big_vision_vit[f'params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/key_ln/scale'])\n",
    "            blk.attn.k_norm.bias.data = torch.zeros_like(blk.attn.k_norm.bias)  # Set bias to zero as it's not present in the checkpoint\n",
    "        print(f\"Replaced query_ln and key_ln for {len(open_clip_model.visual.trunk.blocks)} blocks.\")\n",
    "\n",
    "        def qknorm_param_are_diff(threshold=1e-6):\n",
    "            q_diffs,k_diffs = [],[]\n",
    "            for i, blk in enumerate(open_clip_model.visual.trunk.blocks):\n",
    "                oc_q = blk.attn.q_norm.weight.data.mean().item()\n",
    "                oc_k = blk.attn.k_norm.weight.data.mean().item()\n",
    "                bv_q = big_vision_vit[f'params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/query_ln/scale'].mean()\n",
    "                bv_k = big_vision_vit[f'params/img/Transformer/encoderblock_{i}/MultiHeadDotProductAttention_0/key_ln/scale'].mean()\n",
    "                q_diff = oc_q - bv_q\n",
    "                k_diff = oc_k - bv_k\n",
    "                q_diffs.append(q_diff)\n",
    "                k_diffs.append(k_diff)\n",
    "            q_mean_diff = np.mean(q_diffs)\n",
    "            k_mean_diff = np.mean(k_diffs)\n",
    "            print(f\"q_norm diff mean: {q_mean_diff:.6f}, std: {np.std(q_diffs):.6f}\")\n",
    "            print(f\"k_norm diff mean: {k_mean_diff:.6f}, std: {np.std(k_diffs):.6f}\")    \n",
    "            return abs(q_mean_diff) > threshold or abs(k_mean_diff) > threshold\n",
    "\n",
    "        if qknorm_param_are_diff():\n",
    "            print(\"query_ln and key_ln parameters are different.\")\n",
    "        else:\n",
    "            print(\"query_ln and key_ln parameters are the same.\")\n",
    "\n",
    "    return open_clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! I changed the load_checkpoint function!!!!\n",
      "\n",
      "qknorm_present: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomTextCLIP(\n",
       "  (visual): TimmModel(\n",
       "    (trunk): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (head): Sequential()\n",
       "  )\n",
       "  (text): TextTransformer(\n",
       "    (token_embedding): Embedding(32000, 768)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (text_projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip_model_type = \"ViT-B-16-SigLIP\" # whatever bv model you want to load, use SigLIP to call\n",
    "model_kwargs = {\n",
    "    'vision_cfg': {\n",
    "    'image_size': 224, \n",
    "    'timm_model_name': 'vit_base_patch16_siglip_224', \n",
    "    'timm_model_pretrained': False, \n",
    "    'timm_pool': 'map', \n",
    "    'timm_proj': 'none'\n",
    "  }, \n",
    "}\n",
    "if backbone.split('_')[0] != 'siglip': model_kwargs['vision_cfg']['timm_pool'] = ''\n",
    "\n",
    "open_clip_model, processor = open_clip.create_model_from_pretrained(\n",
    "    open_clip_model_type,\n",
    "    pretrained=local_np_save_path,\n",
    "    # precision='bf16',\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "open_clip_model = qknorm_update(open_clip_model,np_save_path=local_np_save_path)\n",
    "\n",
    "open_clip_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Consistency: big_vision vs. open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0\n",
      "dummy images initialized\n",
      "open_clip output generated\n",
      "big_vision output generated\n",
      "similarity: 0.99831533\n",
      "test 1\n",
      "dummy images initialized\n",
      "open_clip output generated\n",
      "big_vision output generated\n",
      "similarity: 0.99868137\n",
      "test 2\n",
      "dummy images initialized\n",
      "open_clip output generated\n",
      "big_vision output generated\n",
      "similarity: 0.9961379\n",
      "test 3\n",
      "dummy images initialized\n",
      "open_clip output generated\n",
      "big_vision output generated\n",
      "similarity: 0.99832404\n",
      "test 4\n",
      "dummy images initialized\n",
      "open_clip output generated\n",
      "big_vision output generated\n",
      "similarity: 0.9983085\n",
      "average similarity across 5 tests: 0.9979534149169922\n"
     ]
    }
   ],
   "source": [
    "dtype = 'float32' # float32, bfloat16\n",
    "torch_dtype = torch.bfloat16 if dtype == 'bfloat16' else torch.float32\n",
    "num_tests = 5\n",
    "\n",
    "def torch2jnp(tensor,dtype):\n",
    "    if dtype == 'bfloat16':\n",
    "        temp = tensor.float().numpy()\n",
    "        return jnp.array(temp, dtype='bfloat16')\n",
    "    else:\n",
    "        temp = tensor.numpy()\n",
    "        return jnp.array(temp)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    if a.ndim == 2: \n",
    "        a = a[None]\n",
    "        b = b[None]\n",
    "    def sim(a_i, b_i):\n",
    "        return np.dot(a_i, b_i) / (np.linalg.norm(a_i) * np.linalg.norm(b_i))\n",
    "    sim_list = [sim(a_i, b_i) for a_i, b_i in zip(a[0], b[0])]\n",
    "    return np.mean(sim_list)\n",
    "\n",
    "similarity_list = []\n",
    "for i in range(num_tests):\n",
    "    print(f\"test {i}\")\n",
    "    seed = i\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # initialize dummy image with same values\n",
    "    open_clip_dummy_img = torch.randn(2, 3, 224, 224).to(torch_dtype)\n",
    "    bv_dummy_img = torch2jnp(open_clip_dummy_img.permute(0, 2, 3, 1), dtype)\n",
    "    print('dummy images initialized')\n",
    "\n",
    "    # open_clip_dummy_txt = torch.randint(0, 32000, (2, 64)).to(torch.int64)\n",
    "    # bv_dummy_txt = torch2jnp(open_clip_dummy_txt, 'int32')\n",
    "    # print('dummy texts initialized')\n",
    "\n",
    "    oc_out = open_clip_model(open_clip_dummy_img)[0]\n",
    "    print('open_clip output generated')\n",
    "\n",
    "    if backbone.split('_')[0] == 'cappa':\n",
    "        bv_out = bv_model.apply({\"params\":params}, bv_dummy_img, return_enc_features=True, method = 'encode')[0]\n",
    "    elif backbone.split('_')[0] == 'coca':\n",
    "        bv_out = bv_model.apply({\"params\":params}, bv_dummy_img, return_enc_features=True, method = 'encode')[0]\n",
    "    elif backbone.split('_')[0] == 'clip':\n",
    "        bv_out = bv_model.apply({\"params\":params}, bv_dummy_img)[2]['img/encoded']\n",
    "    elif 'gemma' in backbone:\n",
    "        bv_out = bv_model.apply({\"params\":params}, bv_dummy_img, method='embed_image')[1]['img/pre_logits']\n",
    "    else:\n",
    "        bv_out = bv_model.apply({\"params\":params}, bv_dummy_img)[0]\n",
    "    print('big_vision output generated')\n",
    "    similarity = cosine_similarity(oc_out.float().detach().numpy(), bv_out)\n",
    "    print('similarity:', similarity)\n",
    "    similarity_list.append(similarity)\n",
    "    sum(similarity_list) / num_tests\n",
    "print(f\"average similarity across {num_tests} tests: {sum(similarity_list) / num_tests}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cappa debugging code\n",
    "# seed = 0\n",
    "# torch.manual_seed(seed)\n",
    "# dtype = 'bfloat16' # float32, bfloat16\n",
    "# torch_dtype = torch.bfloat16 if dtype == 'bfloat16' else torch.float32\n",
    "\n",
    "# def torch2jnp(tensor,dtype):\n",
    "#     if dtype == 'bfloat16':\n",
    "#         temp = tensor.float().numpy()\n",
    "#         return jnp.array(temp, dtype='bfloat16')\n",
    "#     else:\n",
    "#         temp = tensor.numpy()\n",
    "#         return jnp.array(temp)\n",
    "\n",
    "# def cosine_similarity(a, b):\n",
    "#     print(f\"shapes: {a.shape}, {b.shape}\")\n",
    "#     print(f\"shapes: {a.squeeze().shape}, {b.squeeze().shape}\")\n",
    "#     def sim(a_i, b_i):\n",
    "#         return np.dot(a_i, b_i) / (np.linalg.norm(a_i) * np.linalg.norm(b_i))\n",
    "#     sim_list = [sim(a_i, b_i) for a_i, b_i in zip(a.squeeze(), b.squeeze())]\n",
    "#     return np.mean(sim_list)\n",
    "\n",
    "\n",
    "# # initialize dummy image with same values\n",
    "# open_clip_dummy_img = torch.randn(1, 3, 224, 224).to(torch_dtype)\n",
    "# bv_dummy_img = torch2jnp(open_clip_dummy_img.permute(0, 2, 3, 1), dtype)\n",
    "# print('dummy images initialized')\n",
    "\n",
    "# oc_out = open_clip_model(open_clip_dummy_img)[0]\n",
    "# bv_out = bv_model.apply({\"params\":params}, bv_dummy_img, return_enc_features=True, method = 'encode')\n",
    "# print('out_similarity:', cosine_similarity(oc_out.float().detach().numpy(), bv_out[0]))\n",
    "\n",
    "# oc_stem = open_clip_model.visual.trunk.patch_embed(open_clip_dummy_img)\n",
    "# bv_stem = bv_out[1]['stem'].reshape(1,-1,768)\n",
    "# stem_similarity = cosine_similarity(oc_stem.float().detach().numpy(), bv_stem)\n",
    "# print(f\"stem_similarity: {stem_similarity}\")\n",
    "\n",
    "# oc_pos = open_clip_model.visual.trunk._pos_embed(oc_stem)\n",
    "# bv_pos = bv_out[1]['with_posemb']\n",
    "# pos_similarity = cosine_similarity(oc_pos.float().detach().numpy(), bv_pos)\n",
    "# print(f\"pos_similarity: {pos_similarity}\")\n",
    "\n",
    "# oc_blocs = open_clip_model.visual.trunk.blocks(oc_pos)\n",
    "# oc_encoded = open_clip_model.visual.trunk.norm(oc_blocs)\n",
    "# bv_encoded = bv_out[1]['encoded']\n",
    "# encoded_similarity = cosine_similarity(oc_encoded.float().detach().numpy(), bv_encoded)\n",
    "# print(f\"encoded_similarity: {encoded_similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
