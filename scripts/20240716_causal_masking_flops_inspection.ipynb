{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                                      Foo Summary                                                       \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mflops  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams               \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│                 │ Foo                  │ - \u001b[2mfloat32\u001b[0m[1,16,96]   │ \u001b[2mfloat32\u001b[0m[1,16,96]   │ 1296030 │                       │\n",
      "│                 │                      │ - is_causal: True    │                    │         │                       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention       │ MultiHeadDotProduct… │ - \u001b[2mfloat32\u001b[0m[1,16,96]   │ \u001b[2mfloat32\u001b[0m[1,16,96]   │ 1295518 │                       │\n",
      "│                 │                      │ - \u001b[2mfloat32\u001b[0m[1,16,96]   │                    │         │                       │\n",
      "│                 │                      │ - \u001b[2mfloat32\u001b[0m[1,16,96]   │                    │         │                       │\n",
      "│                 │                      │ - mask:              │                    │         │                       │\n",
      "│                 │                      │ \u001b[2mfloat32\u001b[0m[1,1,16,16]   │                    │         │                       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention/query │ DenseGeneral         │ \u001b[2mfloat32\u001b[0m[1,16,96]     │ \u001b[2mfloat32\u001b[0m[1,16,12,8] │ 296448  │ bias: \u001b[2mfloat32\u001b[0m[12,8]   │\n",
      "│                 │                      │                      │                    │         │ kernel:               │\n",
      "│                 │                      │                      │                    │         │ \u001b[2mfloat32\u001b[0m[96,12,8]      │\n",
      "│                 │                      │                      │                    │         │                       │\n",
      "│                 │                      │                      │                    │         │ \u001b[1m9,312 \u001b[0m\u001b[1;2m(37.2 KB)\u001b[0m       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention/key   │ DenseGeneral         │ \u001b[2mfloat32\u001b[0m[1,16,96]     │ \u001b[2mfloat32\u001b[0m[1,16,12,8] │ 296448  │ bias: \u001b[2mfloat32\u001b[0m[12,8]   │\n",
      "│                 │                      │                      │                    │         │ kernel:               │\n",
      "│                 │                      │                      │                    │         │ \u001b[2mfloat32\u001b[0m[96,12,8]      │\n",
      "│                 │                      │                      │                    │         │                       │\n",
      "│                 │                      │                      │                    │         │ \u001b[1m9,312 \u001b[0m\u001b[1;2m(37.2 KB)\u001b[0m       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention/value │ DenseGeneral         │ \u001b[2mfloat32\u001b[0m[1,16,96]     │ \u001b[2mfloat32\u001b[0m[1,16,12,8] │ 296448  │ bias: \u001b[2mfloat32\u001b[0m[12,8]   │\n",
      "│                 │                      │                      │                    │         │ kernel:               │\n",
      "│                 │                      │                      │                    │         │ \u001b[2mfloat32\u001b[0m[96,12,8]      │\n",
      "│                 │                      │                      │                    │         │                       │\n",
      "│                 │                      │                      │                    │         │ \u001b[1m9,312 \u001b[0m\u001b[1;2m(37.2 KB)\u001b[0m       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention/out   │ DenseGeneral         │ \u001b[2mfloat32\u001b[0m[1,16,12,8]   │ \u001b[2mfloat32\u001b[0m[1,16,96]   │ 296448  │ bias: \u001b[2mfloat32\u001b[0m[96]     │\n",
      "│                 │                      │                      │                    │         │ kernel:               │\n",
      "│                 │                      │                      │                    │         │ \u001b[2mfloat32\u001b[0m[12,8,96]      │\n",
      "│                 │                      │                      │                    │         │                       │\n",
      "│                 │                      │                      │                    │         │ \u001b[1m9,312 \u001b[0m\u001b[1;2m(37.2 KB)\u001b[0m       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m  Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m37,248 \u001b[0m\u001b[1;2m(149.0 KB)\u001b[0m\u001b[1m    \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────────────┴──────────────────────┴──────────────────────┴────────────────────┴─────────┴───────────────────────┘\n",
      "\u001b[1m                                                                                                                        \u001b[0m\n",
      "\u001b[1m                                          Total Parameters: 37,248 \u001b[0m\u001b[1;2m(149.0 KB)\u001b[0m\u001b[1m                                           \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import flax.linen as nn\n",
    "import jax, jax.numpy as jnp\n",
    "\n",
    "def _new_get_flops(fn, *args, **kwargs):\n",
    "  e = jax.jit(fn).lower(*args, **kwargs)\n",
    "  cost = e.compile().cost_analysis()[0]\n",
    "  if cost is None:\n",
    "    return 0\n",
    "  flops = int(cost['flops']) if 'flops' in cost else 0\n",
    "  return flops\n",
    "\n",
    "nn.summary._get_flops = _new_get_flops\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 16\n",
    "dim = 96\n",
    "head = 12\n",
    "x = jnp.ones((batch_size, seq_len, dim))\n",
    "\n",
    "\n",
    "class Foo(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, x,is_causal):\n",
    "    input_ids = jnp.ones((x.shape[0],x.shape[1]))\n",
    "    mask = nn.make_causal_mask(input_ids) if is_causal else None\n",
    "    y = nn.MultiHeadDotProductAttention(\n",
    "        num_heads=head,\n",
    "        qkv_features=dim,\n",
    "        out_features=dim,\n",
    "        kernel_init=nn.initializers.xavier_uniform(),\n",
    "        deterministic=False,\n",
    "        name='attention',\n",
    "        )(x, x, x, mask=mask)\n",
    "    return y\n",
    "\n",
    "print(Foo().tabulate(jax.random.key(0), x, is_causal=True,compute_flops=True, console_kwargs={'width': 120}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 16, 96),\n",
       " Array([[[-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ],\n",
       "         [-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ],\n",
       "         [-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ],\n",
       "         ...,\n",
       "         [-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ],\n",
       "         [-0.66381943,  0.40154403,  0.9372309 , ..., -0.14170712,\n",
       "           1.5388081 ,  1.1570458 ],\n",
       "         [-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ]]], dtype=float32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Foo()\n",
    "params = model.init(jax.random.PRNGKey(0), x, is_causal=True)\n",
    "y = model.apply(params, x, is_causal=True)\n",
    "y.shape,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem: discovering that causal and fully visible masking have same flops\n",
    "- Attempt: changing the attention calculation mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Union, overload\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from flax.linen.module import Module, compact, merge_param\n",
    "from flax.typing import (\n",
    "  Array,\n",
    "  PRNGKey,\n",
    "  Dtype,\n",
    "  Shape as Shape,\n",
    "  Initializer,\n",
    "  PrecisionLike,\n",
    "  DotGeneralT,\n",
    ")\n",
    "\n",
    "def new_dot_product_attention_weights(\n",
    "    query: Array,\n",
    "    key: Array,\n",
    "    bias: Optional[Array] = None,\n",
    "    mask: Optional[Array] = None,\n",
    "    broadcast_dropout: bool = True,\n",
    "    dropout_rng: Optional[PRNGKey] = None,\n",
    "    dropout_rate: float = 0.0,\n",
    "    deterministic: bool = False,\n",
    "    dtype: Optional[Dtype] = None,\n",
    "    precision: PrecisionLike = None,\n",
    "    module: Optional[Module] = None,\n",
    "    force_fp32_for_softmax: bool = False,\n",
    "    einsum_dot_general: Callable[..., Array] = jax.lax.dot_general,\n",
    "):\n",
    "  query, key = promote_dtype(query, key, dtype=dtype)\n",
    "  dtype = query.dtype\n",
    "\n",
    "  assert query.ndim == key.ndim, 'q, k must have same rank.'\n",
    "  assert query.shape[:-3] == key.shape[:-3], 'q, k batch dims must match.'\n",
    "  assert query.shape[-2] == key.shape[-2], 'q, k num_heads must match.'\n",
    "  assert query.shape[-1] == key.shape[-1], 'q, k depths must match.'\n",
    "\n",
    "  # calculate attention matrix\n",
    "  depth = query.shape[-1]\n",
    "  query = query / jnp.sqrt(depth).astype(dtype)\n",
    "  if mask is None:\n",
    "    # attn weight shape is (batch..., num_heads, q_length, kv_length)\n",
    "    attn_weights = jnp.einsum(\n",
    "        '...qhd,...khd->...hqk',\n",
    "        query,\n",
    "        key,\n",
    "        precision=precision,\n",
    "        _dot_general=einsum_dot_general,\n",
    "    )\n",
    "  else:\n",
    "    big_neg = jnp.finfo(dtype).min\n",
    "    bs,seq_len,head,dim = query.shape\n",
    "    attn_weights = jnp.ones((bs, head, seq_len, key.shape[1]))*big_neg\n",
    "    for i in range(seq_len):\n",
    "      # print(f\"i = {i}\")\n",
    "      temp_query = query[:,i:,:,:] # (bs, seq_len-i, head, dim)\n",
    "      temp_key = key[:,i:i+1,:,:] # (bs, 1, head, dim)\n",
    "      temp_attn_weights = jnp.einsum(\n",
    "          '...qhd,...khd->...hqk',\n",
    "          temp_query,\n",
    "          temp_key,\n",
    "          precision=precision,\n",
    "          _dot_general=einsum_dot_general,\n",
    "      ) # (bs, head, seq_len-i, key.shape[1])\n",
    "      # print(f\"temp_query.shape = {temp_query.shape}\")\n",
    "      # print(f\"temp_key.shape = {temp_key.shape}\")\n",
    "      # print(f\"temp_attn_weights.shape = {temp_attn_weights.shape}\")\n",
    "      attn_weights = attn_weights.at[:,:,i:,i:i+1].set(temp_attn_weights)\n",
    "\n",
    "\n",
    "\n",
    "  # apply attention bias: masking, dropout, proximity bias, etc.\n",
    "  if bias is not None:\n",
    "    attn_weights = attn_weights + bias\n",
    "  # # apply attention mask\n",
    "  # if mask is not None:\n",
    "  #   big_neg = jnp.finfo(dtype).min\n",
    "  #   attn_weights = jnp.where(mask, attn_weights, big_neg)\n",
    "  # normalize the attention weights\n",
    "  if force_fp32_for_softmax and dtype != jnp.float32:\n",
    "    attn_weights = jax.nn.softmax(attn_weights.astype(jnp.float32))\n",
    "  else:\n",
    "    attn_weights = jax.nn.softmax(attn_weights).astype(dtype)\n",
    "\n",
    "  if module:\n",
    "    module.sow('intermediates', 'attention_weights', attn_weights)\n",
    "\n",
    "  # apply attention dropout\n",
    "  if not deterministic and dropout_rate > 0.0:\n",
    "    keep_prob = 1.0 - dropout_rate\n",
    "    if broadcast_dropout:\n",
    "      # dropout is broadcast across the batch + head dimensions\n",
    "      dropout_shape = tuple([1] * (key.ndim - 2)) + attn_weights.shape[-2:]\n",
    "      keep = random.bernoulli(dropout_rng, keep_prob, dropout_shape)  # type: ignore\n",
    "    else:\n",
    "      keep = random.bernoulli(dropout_rng, keep_prob, attn_weights.shape)  # type: ignore\n",
    "    multiplier = keep.astype(dtype) / jnp.asarray(keep_prob, dtype=dtype)\n",
    "    attn_weights = attn_weights * multiplier\n",
    "\n",
    "  return attn_weights\n",
    "\n",
    "\n",
    "nn.attention.dot_product_attention_weights = new_dot_product_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                                      Foo Summary                                                       \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mflops  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams               \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│                 │ Foo                  │ - \u001b[2mfloat32\u001b[0m[1,16,96]   │ \u001b[2mfloat32\u001b[0m[1,16,96]   │ 1277568 │                       │\n",
      "│                 │                      │ - is_causal: True    │                    │         │                       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention       │ MultiHeadDotProduct… │ - \u001b[2mfloat32\u001b[0m[1,16,96]   │ \u001b[2mfloat32\u001b[0m[1,16,96]   │ 1277568 │                       │\n",
      "│                 │                      │ - \u001b[2mfloat32\u001b[0m[1,16,96]   │                    │         │                       │\n",
      "│                 │                      │ - \u001b[2mfloat32\u001b[0m[1,16,96]   │                    │         │                       │\n",
      "│                 │                      │ - mask:              │                    │         │                       │\n",
      "│                 │                      │ \u001b[2mfloat32\u001b[0m[1,1,16,16]   │                    │         │                       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention/query │ DenseGeneral         │ \u001b[2mfloat32\u001b[0m[1,16,96]     │ \u001b[2mfloat32\u001b[0m[1,16,12,8] │ 296448  │ bias: \u001b[2mfloat32\u001b[0m[12,8]   │\n",
      "│                 │                      │                      │                    │         │ kernel:               │\n",
      "│                 │                      │                      │                    │         │ \u001b[2mfloat32\u001b[0m[96,12,8]      │\n",
      "│                 │                      │                      │                    │         │                       │\n",
      "│                 │                      │                      │                    │         │ \u001b[1m9,312 \u001b[0m\u001b[1;2m(37.2 KB)\u001b[0m       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention/key   │ DenseGeneral         │ \u001b[2mfloat32\u001b[0m[1,16,96]     │ \u001b[2mfloat32\u001b[0m[1,16,12,8] │ 296448  │ bias: \u001b[2mfloat32\u001b[0m[12,8]   │\n",
      "│                 │                      │                      │                    │         │ kernel:               │\n",
      "│                 │                      │                      │                    │         │ \u001b[2mfloat32\u001b[0m[96,12,8]      │\n",
      "│                 │                      │                      │                    │         │                       │\n",
      "│                 │                      │                      │                    │         │ \u001b[1m9,312 \u001b[0m\u001b[1;2m(37.2 KB)\u001b[0m       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention/value │ DenseGeneral         │ \u001b[2mfloat32\u001b[0m[1,16,96]     │ \u001b[2mfloat32\u001b[0m[1,16,12,8] │ 296448  │ bias: \u001b[2mfloat32\u001b[0m[12,8]   │\n",
      "│                 │                      │                      │                    │         │ kernel:               │\n",
      "│                 │                      │                      │                    │         │ \u001b[2mfloat32\u001b[0m[96,12,8]      │\n",
      "│                 │                      │                      │                    │         │                       │\n",
      "│                 │                      │                      │                    │         │ \u001b[1m9,312 \u001b[0m\u001b[1;2m(37.2 KB)\u001b[0m       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│ attention/out   │ DenseGeneral         │ \u001b[2mfloat32\u001b[0m[1,16,12,8]   │ \u001b[2mfloat32\u001b[0m[1,16,96]   │ 296448  │ bias: \u001b[2mfloat32\u001b[0m[96]     │\n",
      "│                 │                      │                      │                    │         │ kernel:               │\n",
      "│                 │                      │                      │                    │         │ \u001b[2mfloat32\u001b[0m[12,8,96]      │\n",
      "│                 │                      │                      │                    │         │                       │\n",
      "│                 │                      │                      │                    │         │ \u001b[1m9,312 \u001b[0m\u001b[1;2m(37.2 KB)\u001b[0m       │\n",
      "├─────────────────┼──────────────────────┼──────────────────────┼────────────────────┼─────────┼───────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m  Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m37,248 \u001b[0m\u001b[1;2m(149.0 KB)\u001b[0m\u001b[1m    \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────────────┴──────────────────────┴──────────────────────┴────────────────────┴─────────┴───────────────────────┘\n",
      "\u001b[1m                                                                                                                        \u001b[0m\n",
      "\u001b[1m                                          Total Parameters: 37,248 \u001b[0m\u001b[1;2m(149.0 KB)\u001b[0m\u001b[1m                                           \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(Foo().tabulate(jax.random.key(0), x, is_causal=True,compute_flops=True, console_kwargs={'width': 120}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 16, 96),\n",
       " Array([[[-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ],\n",
       "         [-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ],\n",
       "         [-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ],\n",
       "         ...,\n",
       "         [-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ],\n",
       "         [-0.66381943,  0.40154403,  0.9372309 , ..., -0.14170712,\n",
       "           1.5388081 ,  1.1570458 ],\n",
       "         [-0.66383624,  0.40252024,  0.93283004, ..., -0.14342627,\n",
       "           1.5304804 ,  1.1549087 ]]], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Foo()\n",
    "params = model.init(jax.random.PRNGKey(0), x, is_causal=True)\n",
    "y = model.apply(params, x, is_causal=True)\n",
    "y.shape,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Q*K is a really small component of FLOPS in MSA... STUPID! Wasted your 1 hour!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
